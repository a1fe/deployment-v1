"""
Celery signals handlers for HR Analysis system

–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞, –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á.
"""

import logging
import time
from datetime import datetime
from typing import Dict, Any, Optional
from celery.signals import (
    worker_ready, worker_shutting_down, worker_process_init,
    task_failure, task_success, task_retry, task_prerun, task_postrun,
    before_task_publish, after_task_publish
)

# Configure logging
logger = logging.getLogger(__name__)

# Import monitoring system
try:
    from celery_app.monitoring import monitor
    MONITORING_ENABLED = True
except ImportError:
    logger.warning("Monitoring system not available")
    MONITORING_ENABLED = False
    monitor = None

# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (legacy fallback)
task_metrics: Dict[str, Any] = {
    'total_tasks': 0,
    'successful_tasks': 0,
    'failed_tasks': 0,
    'retried_tasks': 0,
    'start_time': time.time()
}

# Task execution tracking
task_execution_times: Dict[str, float] = {}


@worker_ready.connect
def worker_ready_handler(sender, **kwargs):
    """Called when worker is ready to receive tasks"""
    worker_name = sender.hostname
    logger.info(f"üöÄ Worker {worker_name} is ready to process tasks")
    logger.info(f"   Worker PID: {kwargs.get('pid', 'unknown')}")
    logger.info(f"   Queues: {getattr(sender, 'task_routes', {})}")
    
    if MONITORING_ENABLED and monitor:
        monitor.update_worker_status(worker_name, 'online')


@worker_process_init.connect
def worker_process_init_handler(sender, **kwargs):
    """Called when worker process starts"""
    logger.info(f"üîß Worker process {sender.hostname} initialized")


@worker_shutting_down.connect  
def worker_shutting_down_handler(sender, **kwargs):
    """Called when worker is shutting down"""
    worker_name = sender.hostname
    logger.info(f"üõë Worker {worker_name} is shutting down gracefully")
    logger.info(f"üìä Final metrics: {task_metrics}")
    
    if MONITORING_ENABLED and monitor:
        monitor.update_worker_status(worker_name, 'offline')


@before_task_publish.connect
def before_task_publish_handler(sender=None, headers=None, body=None, **kwargs):
    """Called before task is published to broker"""
    task_name = headers.get('task', 'unknown') if headers else 'unknown'
    logger.debug(f"üì§ Publishing task: {task_name}")


@after_task_publish.connect
def after_task_publish_handler(sender=None, headers=None, body=None, **kwargs):
    """Called after task is published to broker"""
    task_name = headers.get('task', 'unknown') if headers else 'unknown'
    logger.debug(f"‚úÖ Task published: {task_name}")


@task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """Called before task starts executing"""
    task_metrics['total_tasks'] += 1
    task_name = getattr(task, 'name', sender or 'unknown')
    
    # Record task start time
    if task_id:
        task_execution_times[str(task_id)] = time.time()
    
    logger.info(f"‚ñ∂Ô∏è Starting task {task_name} (ID: {task_id})")
    logger.debug(f"   Args: {args}, Kwargs: {kwargs}")
    
    if MONITORING_ENABLED and monitor and task_id:
        monitor.record_task_start(task_name, str(task_id))


@task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, 
                        retval=None, state=None, **kwds):
    """Called after task finishes (success or failure)"""
    task_name = getattr(task, 'name', sender or 'unknown')
    
    # Calculate execution time
    start_time = task_execution_times.pop(str(task_id), None) if task_id else None
    execution_time = time.time() - start_time if start_time else 0
    
    logger.info(f"‚èπÔ∏è Task {task_name} finished (ID: {task_id}, State: {state})")
    if execution_time > 0:
        logger.info(f"   Duration: {execution_time:.2f}s")


@task_success.connect
def task_success_handler(sender=None, result=None, **kwargs):
    """Called when task succeeds"""
    task_metrics['successful_tasks'] += 1
    task_name = getattr(sender, 'name', str(sender) if sender else 'unknown')
    task_id = kwargs.get('task_id', 'unknown')
    
    logger.info(f"‚úÖ Task {task_name} completed successfully (ID: {task_id})")
    logger.debug(f"   Result type: {type(result).__name__}")
    
    if MONITORING_ENABLED and monitor:
        # Get execution time from recorded times
        start_time = task_execution_times.get(str(task_id))
        execution_time = time.time() - start_time if start_time else 0
        monitor.record_task_success(task_name, str(task_id), execution_time)


@task_failure.connect
def task_failure_handler(sender=None, task_id=None, exception=None, einfo=None, **kwargs):
    """Enhanced task failure handler with monitoring integration"""
    task_metrics['failed_tasks'] += 1
    task_name = getattr(sender, 'name', str(sender) if sender else 'unknown')
    
    logger.error(f"‚ùå Task {task_name} failed (ID: {task_id}): {exception}")
    logger.error(f"   Exception type: {type(exception).__name__}")
    
    if einfo:
        logger.error(f"   Traceback: {str(einfo)[:500]}...")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
    critical_exceptions = ('DatabaseError', 'ConnectionError', 'TimeoutError')
    if any(exc in str(type(exception)) for exc in critical_exceptions):
        logger.critical(f"üö® Critical error in task {task_id}: {exception}")
    
    if MONITORING_ENABLED and monitor and task_id:
        monitor.record_task_failure(task_name, str(task_id), str(exception))


@task_retry.connect
def task_retry_handler(sender=None, task_id=None, reason=None, einfo=None, **kwargs):
    """Called when task is retried"""
    task_metrics['retried_tasks'] += 1
    task_name = getattr(sender, 'name', str(sender) if sender else 'unknown')
    retry_count = kwargs.get('request', {}).get('retries', 0)
    
    logger.warning(f"üîÑ Task {task_name} is being retried (ID: {task_id})")
    logger.warning(f"   Retry reason: {reason}")
    
    if einfo:
        logger.warning(f"   Error info: {str(einfo)[:200]}...")
    
    if MONITORING_ENABLED and monitor and task_id:
        monitor.record_task_retry(task_name, str(task_id), retry_count, str(reason))


def get_task_metrics() -> Dict[str, Any]:
    """Get current task metrics"""
    uptime = time.time() - task_metrics['start_time']
    return {
        **task_metrics,
        'uptime_seconds': uptime,
        'success_rate': (
            task_metrics['successful_tasks'] / max(task_metrics['total_tasks'], 1) * 100
        )
    }


def register_signals(app):
    """
    –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Celery.
    –†–∞—Å—à–∏—Ä—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏:
    - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–≤—Ç–æ—Ä –∑–∞–¥–∞—á –ø—Ä–∏ —Å–±–æ–µ
    - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π
    - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é
    
    Args:
        app: —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Celery
    """
    logger.info("üîÑ –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤ Celery")
    
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–≤—Ç–æ—Ä–∞ –∑–∞–¥–∞—á –ø—Ä–∏ —Å–±–æ–µ
    task_retries = app.conf.get('task_default_retry_delay', 60)
    max_retries = app.conf.get('task_max_retries', 3)
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–≤—Ç–æ—Ä –∑–∞–¥–∞—á –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
    from celery.signals import task_failure
    
    @task_failure.connect
    def handle_failure(sender=None, task_id=None, exception=None, args=None, kwargs=None, traceback=None, einfo=None, **kwds):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–≤—Ç–æ—Ä –∑–∞–¥–∞—á –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö"""
        # –°–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫, –ø—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–Ω–æ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –∑–∞–¥–∞—á—É
        retriable_errors = (
            'ConnectionError', 'TimeoutError', 'DatabaseError',
            'OperationalError', 'NetworkError'
        )
        
        # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è, –ª–æ–≥–∏—Ä—É–µ–º
        if exception and any(err in type(exception).__name__ for err in retriable_errors):
            logger.warning(f"üîÑ –ó–∞–¥–∞—á–∞ {task_id} —É–ø–∞–ª–∞ —Å –ø–æ–≤—Ç–æ—Ä–∞–µ–º–æ–π –æ—à–∏–±–∫–æ–π: {exception}")
            # Note: Retry logic should be handled by task decorators, not here
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ custom retry —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    def custom_retry(task, exc=None, throw=True, eta=None, countdown=None, 
                  max_retries=None, **options):
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–≤—Ç–æ—Ä–∞ –∑–∞–¥–∞—á —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π
        –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—Å–æ–±—ã—Ö —Å–ª—É—á–∞–µ–≤
        """
        if max_retries is None:
            max_retries = task.max_retries
            
        current_retry = task.request.retries
        
        # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–æ–≤—Ç–æ—Ä–∞ (1m, 5m, 15m)
        retry_delays = [60, 300, 900, 1800]
        
        if countdown is None and current_retry < len(retry_delays):
            countdown = retry_delays[current_retry]
        elif countdown is None:
            countdown = retry_delays[-1]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∞ "thundering herd"
        import random
        jitter = random.uniform(0.8, 1.2)
        countdown = int(countdown * jitter)
        
        logger.info(
            f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–¥–∞—á–∏ {task.name} ({current_retry+1}/{max_retries}) "
            f"—á–µ—Ä–µ–∑ {countdown}s"
        )
        
        return task.retry(
            exc=exc, throw=throw, eta=eta, 
            countdown=countdown, max_retries=max_retries,
            **options
        )
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º custom retry —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
    app.Task.retry = custom_retry
    
    logger.info("‚úÖ –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤ Celery –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã")
    
    return app
