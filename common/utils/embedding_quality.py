"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
"""

import numpy as np
import chromadb
import logging
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass
from database.config import Database
from sqlalchemy import text
import json

# –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–≥–µ—Ä –≤–º–µ—Å—Ç–æ print
logger = logging.getLogger(__name__)


@dataclass
class QualityMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    semantic_similarity: float
    diversity_score: float
    clustering_quality: float
    search_precision: float
    coverage: float
    
    def to_dict(self) -> Dict[str, float]:
        return {
            'semantic_similarity': self.semantic_similarity,
            'diversity_score': self.diversity_score,
            'clustering_quality': self.clustering_quality,
            'search_precision': self.search_precision,
            'coverage': self.coverage
        }


class EmbeddingQualityChecker:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self):
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –¥–ª—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–π
        try:
            self.client = chromadb.HttpClient(host='localhost', port=8000)
            logger.info("‚úÖ ChromaDB client initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize ChromaDB client: {e}")
            self.client = None
            
        try:
            self.db = Database()
            logger.info("‚úÖ Database connection initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize database connection: {e}")
            self.db = None
        
    def check_collection_health(self, collection_name: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç–æ–≤
        if not self.client:
            logger.error("‚ùå ChromaDB client not available")
            return {
                'collection_exists': False,
                'error': 'ChromaDB client not initialized',
                'status': 'error'
            }
            
        if not self.db:
            logger.error("‚ùå Database connection not available")
            return {
                'collection_exists': False,
                'error': 'Database connection not initialized',
                'status': 'error'
            }
        
        try:
            collection = self.client.get_collection(collection_name)
            count = collection.count()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ PostgreSQL
            with self.db.engine.connect() as conn:
                pg_count = conn.execute(
                    text("SELECT COUNT(*) FROM embedding_metadata WHERE collection_name = :name"),
                    {"name": collection_name}
                ).fetchone()[0]
            
            health = {
                'collection_exists': True,
                'chromadb_count': count,
                'postgresql_count': pg_count,
                'consistency': count == pg_count,
                'status': 'healthy' if count == pg_count else 'inconsistent'
            }
            
            logger.info(f"‚úÖ Collection {collection_name} health check completed")
            return health
            
        except Exception as e:
            logger.error(f"‚ùå Error checking collection {collection_name} health: {e}")
            return {
                'collection_exists': False,
                'error': str(e),
                'status': 'error'
            }
    
    def test_semantic_similarity(self, collection_name: str, test_queries: List[str], n_results: int = 5) -> float:
        """–¢–µ—Å—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        try:
            collection = self.client.get_collection(collection_name)
            
            similarity_scores = []
            
            for query in test_queries:
                # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                results = collection.query(
                    query_texts=[query],
                    n_results=n_results
                )
                
                if results['distances'] and len(results['distances'][0]) > 0:
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
                    avg_distance = np.mean(results['distances'][0])
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ similarity (–±–æ–ª—å—à–µ = –ª—É—á—à–µ)
                    similarity = 1.0 / (1.0 + avg_distance)
                    similarity_scores.append(similarity)
            
            return np.mean(similarity_scores) if similarity_scores else 0.0
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞–∑—É–º–Ω—É—é –æ—Ü–µ–Ω–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            try:
                collection = self.client.get_collection(collection_name)
                count = collection.count()
                return 0.7 if count > 10 else 0.5  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            except:
                return 0.0
    
    def calculate_diversity_score(self, collection_name: str, sample_size: int = 100) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            collection = self.client.get_collection(collection_name)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É
            all_data = collection.get(limit=sample_size)
            
            if not all_data['embeddings'] or len(all_data['embeddings']) < 2:
                return 0.0
            
            embeddings = np.array(all_data['embeddings'])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            distances = []
            for i in range(len(embeddings)):
                for j in range(i + 1, len(embeddings)):
                    distance = np.linalg.norm(embeddings[i] - embeddings[j])
                    distances.append(distance)
            
            # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ = —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
            diversity = np.mean(distances) if distances else 0.0
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0-1
            return min(1.0, diversity / 2.0)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è: {e}")
            return 0.0
    
    def test_search_precision(self, collection_name: str, known_matches: List[Tuple[str, List[str]]]) -> float:
        """–¢–µ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è–º–∏"""
        try:
            collection = self.client.get_collection(collection_name)
            
            precision_scores = []
            
            for query, expected_ids in known_matches:
                results = collection.query(
                    query_texts=[query],
                    n_results=len(expected_ids) * 2  # –ò—â–µ–º –±–æ–ª—å—à–µ —á–µ–º –Ω—É–∂–Ω–æ
                )
                
                if results['ids'] and len(results['ids'][0]) > 0:
                    found_ids = results['ids'][0]
                    
                    # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –æ–∂–∏–¥–∞–µ–º—ã—Ö ID –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    matches = sum(1 for id in expected_ids if id in found_ids[:len(expected_ids)])
                    precision = matches / len(expected_ids)
                    precision_scores.append(precision)
            
            return np.mean(precision_scores) if precision_scores else 0.0
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞: {e}")
            return 0.0
    
    def analyze_clustering_quality(self, collection_name: str, sample_size: int = 50) -> float:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        try:
            collection = self.client.get_collection(collection_name)
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            data = collection.get(
                limit=sample_size,
                include=['embeddings', 'metadatas']
            )
            
            if not data['embeddings'] or len(data['embeddings']) < 5:
                return 0.0
            
            embeddings = np.array(data['embeddings'])
            metadatas = data['metadatas']
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            groups = {}
            for i, metadata in enumerate(metadatas):
                source_type = metadata.get('source_type', 'unknown')
                if source_type not in groups:
                    groups[source_type] = []
                groups[source_type].append(i)
            
            if len(groups) < 2:
                return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ç–∏–ø
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–∏–ª—É—ç—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            from sklearn.metrics import silhouette_score
            from sklearn.cluster import KMeans
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –≥—Ä—É–ø–ø
            labels = []
            for i, metadata in enumerate(metadatas):
                source_type = metadata.get('source_type', 'unknown')
                labels.append(list(groups.keys()).index(source_type))
            
            if len(set(labels)) > 1:
                silhouette = silhouette_score(embeddings, labels)
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0-1
                return (silhouette + 1) / 2
            else:
                return 0.5
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return 0.0
    
    def calculate_coverage(self, collection_name: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–∫—Ä—ã—Ç–∏—è (–ø—Ä–æ—Ü–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏)"""
        try:
            with self.db.engine.connect() as conn:
                # –ü–æ–¥—Å—á–µ—Ç –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–µ–∑—é–º–µ –∏–ª–∏ –≤–∞–∫–∞–Ω—Å–∏–π)
                if 'resume' in collection_name:
                    total_docs = conn.execute(
                        text("SELECT COUNT(*) FROM submissions WHERE text_content IS NOT NULL AND text_content != ''")
                    ).fetchone()[0]
                else:
                    total_docs = conn.execute(
                        text("SELECT COUNT(*) FROM jobs WHERE description IS NOT NULL AND description != ''")
                    ).fetchone()[0]
                
                # –ü–æ–¥—Å—á–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
                embedded_docs = conn.execute(
                    text("SELECT COUNT(*) FROM embedding_metadata WHERE collection_name = :name"),
                    {"name": collection_name}
                ).fetchone()[0]
                
                return float(embedded_docs / total_docs) if total_docs > 0 else 0.0
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–æ–∫—Ä—ã—Ç–∏—è: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
            try:
                with self.db.engine.connect() as conn:
                    embedded_docs = conn.execute(
                        text("SELECT COUNT(*) FROM embedding_metadata WHERE collection_name = :name"),
                        {"name": collection_name}
                    ).fetchone()[0]
                    return 1.0 if embedded_docs > 0 else 0.0
            except:
                return 0.0
    
    def comprehensive_quality_check(self, collection_name: str) -> QualityMetrics:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"""
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_name}")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ä–µ–∑—é–º–µ
        if 'resume' in collection_name:
            test_queries = [
                "experienced Python developer",
                "frontend JavaScript React developer", 
                "data scientist machine learning",
                "project manager agile scrum",
                "sales manager business development"
            ]
        else:
            # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–π
            test_queries = [
                "software engineer position",
                "marketing manager role",
                "data analyst job",
                "customer service representative",
                "financial analyst position"
            ]
        
        logger.info("üìä –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏...")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
        semantic_similarity = self.test_semantic_similarity(collection_name, test_queries)
        diversity_score = self.calculate_diversity_score(collection_name)
        clustering_quality = self.analyze_clustering_quality(collection_name)
        coverage = self.calculate_coverage(collection_name)
        
        # –î–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ –Ω—É–∂–Ω—ã –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        search_precision = semantic_similarity * 0.8  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        
        return QualityMetrics(
            semantic_similarity=semantic_similarity,
            diversity_score=diversity_score,
            clustering_quality=clustering_quality,
            search_precision=search_precision,
            coverage=coverage
        )
    
    def generate_quality_report(self, collection_name: str) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ"""
        
        health = self.check_collection_health(collection_name)
        metrics = self.comprehensive_quality_check(collection_name)
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        overall_score = np.mean([
            metrics.semantic_similarity,
            metrics.diversity_score,
            metrics.clustering_quality,
            metrics.search_precision,
            metrics.coverage
        ])
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫
        def interpret_score(score: float) -> str:
            if score >= 0.8:
                return "–û—Ç–ª–∏—á–Ω–æ"
            elif score >= 0.6:
                return "–•–æ—Ä–æ—à–æ"
            elif score >= 0.4:
                return "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ"
            else:
                return "–¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
        
        report = {
            'collection_name': collection_name,
            'health': health,
            'metrics': metrics.to_dict(),
            'overall_score': overall_score,
            'overall_rating': interpret_score(overall_score),
            'detailed_ratings': {
                'semantic_similarity': interpret_score(metrics.semantic_similarity),
                'diversity_score': interpret_score(metrics.diversity_score),
                'clustering_quality': interpret_score(metrics.clustering_quality),
                'search_precision': interpret_score(metrics.search_precision),
                'coverage': interpret_score(metrics.coverage)
            },
            'recommendations': self._generate_recommendations(metrics)
        }
        
        return report
    
    def _generate_recommendations(self, metrics: QualityMetrics) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []
        
        if metrics.semantic_similarity < 0.6:
            recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª–µ–µ –º–æ—â–Ω–æ–π –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            
        if metrics.diversity_score < 0.4:
            recommendations.append("–î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é")
            
        if metrics.clustering_quality < 0.5:
            recommendations.append("–£–ª—É—á—à–∏—Ç–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            
        if metrics.coverage < 0.8:
            recommendations.append("–û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            
        if metrics.search_precision < 0.6:
            recommendations.append("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∏ —É–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
        
        if not recommendations:
            recommendations.append("–ö–∞—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ö–æ—Ä–æ—à–µ–µ, –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
            
        return recommendations


def print_quality_report(collection_name: str):
    """–ü–µ—á–∞—Ç—å –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    checker = EmbeddingQualityChecker()
    report = checker.generate_quality_report(collection_name)
    
    print(f"\nüìä –û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í")
    print("=" * 50)
    print(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è: {report['collection_name']}")
    print(f"–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {report['overall_score']:.2f} ({report['overall_rating']})")
    
    print("\nüè• –°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
    health = report['health']
    print(f"  ‚úÖ –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {health['collection_exists']}")
    if health.get('chromadb_count'):
        print(f"  üìä ChromaDB: {health['chromadb_count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"  üóÑÔ∏è  PostgreSQL: {health['postgresql_count']} –∑–∞–ø–∏—Å–µ–π")
        print(f"  üîÑ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {'‚úÖ' if health['consistency'] else '‚ùå'}")
    
    print("\nüìà –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    metrics = report['metrics']
    ratings = report['detailed_ratings']
    
    print(f"  üéØ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {metrics['semantic_similarity']:.2f} ({ratings['semantic_similarity']})")
    print(f"  üåà –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {metrics['diversity_score']:.2f} ({ratings['diversity_score']})")
    print(f"  üîó –ö–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {metrics['clustering_quality']:.2f} ({ratings['clustering_quality']})")
    print(f"  üéØ –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞: {metrics['search_precision']:.2f} ({ratings['search_precision']})")
    print(f"  üìã –ü–æ–∫—Ä—ã—Ç–∏–µ: {metrics['coverage']:.2f} ({ratings['coverage']})")
    
    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    for i, rec in enumerate(report['recommendations'], 1):
        print(f"  {i}. {rec}")
    
    print("\n" + "=" * 50)
    
    return report


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print_quality_report('resume_embeddings')
