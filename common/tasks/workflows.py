"""
Workflow tasks for orchestrating the processing pipeline
"""

from typing import Dict, Any
from celery.utils.log import get_task_logger
from celery import group, chain, chord, signature
from celery_app.celery_app import celery_app
from database.operations.embedding_operations import embedding_crud
from database.config import database

from celery_app.celery_app import celery_app

logger = get_task_logger(__name__)


@celery_app.task(
    bind=True,
    name='tasks.workflows.run_full_processing_pipeline',
    soft_time_limit=3600,
    time_limit=4200,
    max_retries=2
)
def run_full_processing_pipeline(self, previous_results=None) -> Dict[str, Any]:
    """
    –ü–æ–ª–Ω—ã–π chain-based pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: –ø–æ–ª—É—á–µ–Ω–∏–µ ‚Üí –ø–∞—Ä—Å–∏–Ω–≥ ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Üí —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Celery chain –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏.
    –ö–∞–∂–¥—ã–π —ç—Ç–∞–ø –ø–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–π —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä previous_results.
    
    Args:
        previous_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç—Ç–∞–ø–æ–≤ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å chain)
        
    Returns:
        Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∑–∞–ø—É—â–µ–Ω–Ω–æ–º pipeline
    """
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ chain-based pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    # –ï—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞, –ª–æ–≥–∏—Ä—É–µ–º –∏—Ö
    if previous_results:
        logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞: {previous_results}")
    
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏
        from tasks.fillout_tasks import fetch_resume_data, fetch_company_data
        from tasks.parsing_tasks import parse_resume_text, parse_job_text
        from tasks.embedding_tasks import generate_resume_embeddings, generate_job_embeddings
        from tasks.reranking_tasks import rerank_resumes_for_job, rerank_jobs_for_resume
        
        def flatten(items):
            for x in items:
                if isinstance(x, (list, tuple)):
                    yield from flatten(x)
                else:
                    yield x

        # –°–æ–∑–¥–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π chain pipeline
        # –ö–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–π —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä previous_results
        embedding_group = group([
            generate_resume_embeddings.s(),
            generate_job_embeddings.s()
        ])
        pipeline_chain = chain(
            group([
                fetch_resume_data.s(),
                fetch_company_data.s()
            ]),
            group([
                parse_resume_text.s(),
                parse_job_text.s()
            ]),
            chord(embedding_group, launch_reranking_tasks.s())
        )
        # –ó–∞–ø—É—Å–∫–∞–µ–º chain –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º AsyncResult –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –≤–æ—Ä–∫–µ—Ä–∞
        result = pipeline_chain.apply_async()
        result_id = getattr(result, 'id', 'unknown')
        logger.info(f"‚úÖ Chain pipeline –∑–∞–ø—É—â–µ–Ω —É—Å–ø–µ—à–Ω–æ: ID={result_id}")
        logger.info("üìã –≠—Ç–∞–ø—ã pipeline:")
        logger.info("  1. –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: fetch_resume_data + fetch_company_data")
        logger.info("  2. –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤: parse_resume_text + parse_job_text")
        logger.info("  3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: generate_all_embeddings")
        logger.info("  4. –†–µ—Ä–∞–Ω–∫–∏–Ω–≥: rerank_resumes_for_job + rerank_jobs_for_resume")
        logger.info(f"üîó –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏")
        return {
            'status': 'pipeline_started',
            'pipeline_id': result_id,
            'message': 'Chain pipeline –∑–∞–ø—É—â–µ–Ω, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏',
            'stages': [
                'data_fetching',
                'text_parsing', 
                'embedding_generation',
                'reranking'
            ],
            'tracking': '–û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ Flower: http://localhost:5555'
        }
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ pipeline: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'processed_files': 0,
            'error_files': 0,
            'processed_embeddings': 0,
            'error_embeddings': 0
        }


@celery_app.task(
    bind=True,
    name='tasks.workflows.run_parsing_only',
    soft_time_limit=1800,
    time_limit=2100,
    max_retries=2
)
def run_parsing_only(self, previous_results=None) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–π
    
    Args:
        previous_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç—Ç–∞–ø–æ–≤ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å chain)
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
    """
    logger.info("üìÑ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–π")
    
    if previous_results:
        logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞: {previous_results}")
    
    try:
        from tasks.parsing_tasks import parse_resume_text, parse_job_text
        
        # –°–æ–∑–¥–∞–µ–º pipeline —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        parsing_chain = group([
            parse_resume_text.s(previous_results),
            parse_job_text.s(previous_results)
        ])
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
        result = parsing_chain.apply_async()
        result_id = getattr(result, 'id', 'unknown')
        
        logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω: ID={result_id}")
        
        return {
            'status': 'parsing_started',
            'pipeline_id': result_id,
            'message': '–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞–ø—É—â–µ–Ω',
            'stages': ['text_parsing'],
            'tracking': '–û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ Flower: http://localhost:5555'
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return {
            'status': 'error',
            'error': str(e)
        }


@celery_app.task(
    bind=True,
    name='tasks.workflows.run_embeddings_only',
    soft_time_limit=1800,
    time_limit=2100,
    max_retries=2
)
def run_embeddings_only(self, previous_results=None) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    Args:
        previous_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç—Ç–∞–ø–æ–≤ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å chain)
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    logger.info("üß† –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    if previous_results:
        logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞: {previous_results}")
    
    try:
        from tasks.embedding_tasks import generate_resume_embeddings, generate_job_embeddings
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ group
        embedding_group = group([
            generate_resume_embeddings.s(previous_results),
            generate_job_embeddings.s(previous_results)
        ])
        result = embedding_group.apply_async()
        result_id = getattr(result, 'id', 'unknown')
        logger.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (group) –∑–∞–ø—É—â–µ–Ω–∞: ID={result_id}")
        return {
            'status': 'embeddings_started',
            'pipeline_id': result_id,
            'message': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (group) –∑–∞–ø—É—â–µ–Ω–∞',
            'stages': ['embedding_generation'],
            'tracking': '–û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ Flower: http://localhost:5555'
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return {
            'status': 'error',
            'error': str(e)
        }


@celery_app.task(
    bind=True,
    name='tasks.workflows.run_reranking_only',
    soft_time_limit=1800,
    time_limit=2100,
    max_retries=2
)
def run_reranking_only(self, previous_results=None) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
    
    Args:
        previous_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç—Ç–∞–ø–æ–≤ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å chain)
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
    """
    logger.info("üîÑ –ó–∞–ø—É—Å–∫ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
    
    if previous_results:
        logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞: {previous_results}")
    
    try:
        # from tasks.reranking_tasks import process_all_reranking  # –û—Ç–∫–ª—é—á–µ–Ω–æ: –∑–∞–¥–∞—á–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥  
        # result = process_all_reranking.apply_async(args=[previous_results])  # –û—Ç–∫–ª—é—á–µ–Ω–æ: –∑–∞–¥–∞—á–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è
        result_id = 'unknown'  # –í—Ä–µ–º–µ–Ω–Ω–æ, –ø–æ–∫–∞ –∑–∞–¥–∞—á–∞ –Ω–µ –±—É–¥–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∞
        
        logger.info(f"‚úÖ –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω: ID={result_id}")
        
        return {
            'status': 'reranking_started',
            'pipeline_id': result_id,
            'message': '–†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω',
            'stages': ['reranking'],
            'tracking': '–û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ Flower: http://localhost:5555'
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {e}")
        return {
            'status': 'error',
            'error': str(e)
        }


@celery_app.task(
    bind=True,
    name='tasks.workflows.launch_reranking_tasks',
    soft_time_limit=1800,
    time_limit=2100,
    max_retries=2
)
def launch_reranking_tasks(self, results) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –ø–æ –≤—Å–µ–º source_id –∏–∑ embedding_metadata
    """

    from celery_app.queue_names import RERANKING_QUEUE
    from utils.chroma_config import ChromaConfig
    logger.info("üîÑ [RERANK] –ó–∞–¥–∞—á–∞ launch_reranking_tasks –≤—ã–∑–≤–∞–Ω–∞!")
    db = database.get_session()
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ source_id –¥–ª—è —Ä–µ–∑—é–º–µ
        resume_ids = [e.source_id for e in db.query(embedding_crud.model).filter(
            embedding_crud.model.collection_name == ChromaConfig.RESUME_COLLECTION,
            embedding_crud.model.source_type == 'resume'
        ).all()]
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ source_id –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–π
        job_ids = [e.source_id for e in db.query(embedding_crud.model).filter(
            embedding_crud.model.collection_name == ChromaConfig.JOB_COLLECTION,
            embedding_crud.model.source_type == 'job_description'
        ).all()]
        logger.info(f"[RERANK] –ù–∞–π–¥–µ–Ω–æ {len(resume_ids)} resume_ids: {resume_ids}")
        logger.info(f"[RERANK] –ù–∞–π–¥–µ–Ω–æ {len(job_ids)} job_ids: {job_ids}")
        if not resume_ids and not job_ids:
            logger.warning("[RERANK] –ù–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ id –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∞–±–ª–∏—Ü—É embedding_metadata.")
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Celery-–∑–∞–¥–∞—á–∏ –ø–æ –∏–º–µ–Ω–∏
        rerank_resumes_for_job = celery_app.tasks['tasks.reranking_tasks.rerank_resumes_for_job']
        rerank_jobs_for_resume = celery_app.tasks['tasks.reranking_tasks.rerank_jobs_for_resume']
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ —Ä–µ—Ä–∞–Ω–∫–∞ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –æ—á–µ—Ä–µ–¥–∏
        result = group([
            group([rerank_jobs_for_resume.s(sub_id).set(queue=RERANKING_QUEUE) for sub_id in resume_ids]),
            group([rerank_resumes_for_job.s(job_id).set(queue=RERANKING_QUEUE) for job_id in job_ids])
        ]).apply_async()
        logger.info(f"‚úÖ –†–µ—Ä–∞–Ω–∫ –∑–∞–ø—É—â–µ–Ω –¥–ª—è {len(resume_ids)} —Ä–µ–∑—é–º–µ –∏ {len(job_ids)} –≤–∞–∫–∞–Ω—Å–∏–π")
        return {
            'status': 'reranking_started',
            'resume_count': len(resume_ids),
            'job_count': len(job_ids),
            'message': '–†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω –ø–æ –≤—Å–µ–º source_id –∏–∑ embedding_metadata',
            'tracking': f'–û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ Flower: http://localhost:5555/task/{result.id}'
        }
    finally:
        db.close()


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ pipeline (–ø—Ä–æ—Å—Ç—ã–µ shortcuts)
def trigger_full_pipeline():
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ pipeline"""
    return run_full_processing_pipeline.delay()

def trigger_parsing_only():
    """–ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    return run_parsing_only.delay()

def trigger_embeddings_only():
    """–ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    return run_embeddings_only.delay()

def trigger_reranking_only():
    """–ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞"""
    return run_reranking_only.delay()
