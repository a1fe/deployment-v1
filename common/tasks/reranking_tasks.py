"""
Reranking tasks for improving matching quality between resumes and jobs

–ó–∞–¥–∞—á–∞ 4: –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
"""

import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from celery.utils.log import get_task_logger
from dotenv import load_dotenv
from sqlalchemy.orm import Session
from uuid import UUID
from celery.signals import worker_process_init

from common.celery_app.celery_app import celery_app
from common.database.config import database
from common.database.operations.analysis_operations import RerankerAnalysisResultCRUD
from common.database.operations.embedding_operations import embedding_crud
from common.database.operations.candidate_operations import SubmissionCRUD
from common.database.operations.company_operations import JobCRUD
from common.celery_app.queue_names import RERANKING_QUEUE

from common.models.candidates import Submission
from common.models.companies import Job
from common.models.analysis_results import RerankerAnalysisResult
from common.utils.chroma_config import chroma_client, ChromaConfig
from common.utils.reranker_config import get_reranker_client

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

logger = get_task_logger(__name__)

# CRUD —ç–∫–∑–µ–º–ø–ª—è—Ä—ã
analysis_crud = RerankerAnalysisResultCRUD()
submission_crud = SubmissionCRUD()
job_crud = JobCRUD()


@celery_app.task(
    bind=True,
    name='common.tasks.reranking_tasks.rerank_resumes_for_job',
    soft_time_limit=600,
    time_limit=720,
    max_retries=3
)
def rerank_resumes_for_job(self, job_id: int, top_k: int = 50) -> Dict[str, Any]:
    """
    Task 4A: –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Ç–æ–ø-50 —Ä–µ–∑—é–º–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
    
    –ù–∞—Ö–æ–¥–∏—Ç —Ç–æ–ø-50 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∑—é–º–µ —á–µ—Ä–µ–∑ ChromaDB,
    –∑–∞—Ç–µ–º –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –∏—Ö —Å –ø–æ–º–æ—â—å—é BGE Reranker –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ PostgreSQL.
    
    Args:
        job_id: ID –≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∑—é–º–µ
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—é–º–µ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)
    
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
    """
    logger.info(f"üîç –ó–∞–ø—É—Å–∫ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")

    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –µ—Å–ª–∏ job_id –Ω–µ int, –∑–∞–¥–∞—á–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è
    if not isinstance(job_id, int):
        logger.warning(f"‚ö†Ô∏è job_id –ø–µ—Ä–µ–¥–∞–Ω –≤ –Ω–µ–≤–µ—Ä–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: {type(job_id)}. –ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–∞—á–∏.")
        return {
            'status': 'skipped',
            'error': 'job_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å int',
            'processed_matches': 0,
            'error_matches': 0
        }
    
    try:
        db = database.get_session()
        processed_count = 0
        error_count = 0
        
        try:
            # 1. –ü–æ–ª—É—á–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏—é
            job = job_crud.get_by_id(db, job_id)
            if not job:
                logger.error(f"‚ùå –í–∞–∫–∞–Ω—Å–∏—è {job_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return {
                    'status': 'error',
                    'error': f'–í–∞–∫–∞–Ω—Å–∏—è {job_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            
            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏
            if not getattr(job, 'job_description_raw_text', None) or not str(job.job_description_raw_text).strip():
                logger.error(f"‚ùå –£ –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id} –Ω–µ—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
                return {
                    'status': 'error',
                    'error': f'–£ –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id} –Ω–µ—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            
            # 3. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ (–æ–±—Ä–µ–∑–∞–µ–º –¥–æ 32,000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞)
            job_text_full = str(job.job_description_raw_text)
            original_length = len(job_text_full)
            
            if original_length > 32000:
                job_text = job_text_full[:32000]
                logger.info(f"üìè –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id} –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞ —Å {original_length} –¥–æ {len(job_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                job_text = job_text_full
                logger.info(f"üìè –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}: {original_length} —Å–∏–º–≤–æ–ª–æ–≤ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞)")
            
            # 4. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∑—é–º–µ –≤ ChromaDB
            logger.info(f"üîç –ü–æ–∏—Å–∫ —Ç–æ–ø-{top_k} –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∑—é–º–µ –≤ ChromaDB")
            resume_collection = chroma_client.get_resume_collection()
            
            if resume_collection.count() == 0:
                logger.warning("‚ö†Ô∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è —Ä–µ–∑—é–º–µ –ø—É—Å—Ç–∞")
                return {
                    'status': 'completed',
                    'processed_matches': 0,
                    'error_matches': 0,
                    'total_found': 0
                }
            
            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∑—é–º–µ
            search_results = resume_collection.query(
                query_texts=[job_text],
                n_results=min(top_k, resume_collection.count()),
                include=['documents', 'metadatas', 'distances']
            )
            
            if not search_results['documents'] or not search_results['documents'][0]:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∑—é–º–µ")
                return {
                    'status': 'completed',
                    'processed_matches': 0,
                    'error_matches': 0,
                    'total_found': 0
                }
            
            # 5. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
            documents = search_results['documents'][0]
            metadatas = search_results['metadatas'][0]
            distances = search_results['distances'][0]

            # –ü—Ä–æ–≤–µ—Ä–∫–∞: distances –∏ metadatas –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–∞–º–∏
            if not isinstance(distances, list):
                logger.error(f"[FATAL] distances –Ω–µ —Å–ø–∏—Å–æ–∫: {type(distances)}, value={distances}")
                return {
                    'status': 'error',
                    'error': 'distances –Ω–µ —Å–ø–∏—Å–æ–∫',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            if not isinstance(metadatas, list):
                logger.error(f"[FATAL] metadatas –Ω–µ —Å–ø–∏—Å–æ–∫: {type(metadatas)}, value={metadatas}")
                return {
                    'status': 'error',
                    'error': 'metadatas –Ω–µ —Å–ø–∏—Å–æ–∫',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(documents)} —Ä–µ–∑—é–º–µ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
            # 6. –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é BGE Reranker
            reranker = get_reranker_client()
            
            if not reranker.health_check():
                logger.error("‚ùå BGE Reranker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return {
                    'status': 'error',
                    'error': 'BGE Reranker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            
            logger.info(f"üß† –ù–∞—á–∏–Ω–∞–µ–º —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ {len(documents)} —Ä–µ–∑—é–º–µ")
            reranked_results = reranker.rerank_texts(job_text, documents)
            
            if not reranked_results:
                logger.warning("‚ö†Ô∏è –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return {
                    'status': 'completed',
                    'processed_matches': 0,
                    'error_matches': 0,
                    'total_found': len(documents)
                }
            
            # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ PostgreSQL –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(reranked_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
            batch_size = 1  # –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            batch_results = []
            
            for rank_position, (doc_idx, rerank_score) in enumerate(reranked_results, 1):
                try:
                    if doc_idx >= len(metadatas):
                        logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {doc_idx} –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
                        continue
                    
                    metadata = metadatas[doc_idx]
                    source_id = metadata.get('source_id')
                    
                    if not source_id:
                        logger.warning(f"‚ö†Ô∏è –ù–µ—Ç source_id –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {doc_idx}")
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º submission
                    submission = submission_crud.get_by_id(db, source_id)
                    if not submission:
                        logger.warning(f"‚ö†Ô∏è Submission {source_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
                    original_distance = distances[doc_idx] if doc_idx < len(distances) else 0.0
                    original_similarity = max(0.0, 1.0 - original_distance)
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º rerank_score –æ—Ç [-10, +10] –∫ [0, 1]
                    normalized_rerank_score = max(0.0, min(1.0, (rerank_score + 10.0) / 20.0))
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è original similarity –∏ normalized rerank score)
                    final_score = (original_similarity * 0.3) + (normalized_rerank_score * 0.7)
                    score_improvement = normalized_rerank_score - original_similarity
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
                    search_params = {
                        'top_k': top_k,
                        'min_similarity': 0.0,
                        'min_rerank_score': -10.0,
                        'search_type': 'job_to_resumes',
                        'query_text_length': len(job_text),
                        'original_text_length': original_length,
                        'text_truncated': original_length > 32000
                    }
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ workflow
                    workflow_stats = {
                        'total_candidates_found': len(documents),
                        'reranked_candidates': len(reranked_results),
                        'processing_time': datetime.utcnow().isoformat(),
                        'chroma_collection': ChromaConfig.RESUME_COLLECTION
                    }
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –∞–Ω–∞–ª–∏–∑–∞
                    analysis_result = RerankerAnalysisResult(
                        job_id=job_id,
                        submission_id=submission.submission_id,
                        original_similarity=original_similarity,
                        rerank_score=rerank_score,
                        final_score=final_score,
                        score_improvement=score_improvement,
                        rank_position=rank_position,
                        search_params=search_params,
                        reranker_model=reranker.model_name,
                        workflow_stats=workflow_stats,
                        job_title=job.title or 'Unknown',
                        company_id=job.company_id,
                        candidate_name=f"{submission.candidate.first_name or ''} {submission.candidate.last_name or ''}".strip() or 'Unknown',
                        candidate_email=submission.candidate.email or 'Unknown',
                        total_candidates_found=len(documents),
                        analysis_type='job_to_resumes_rerank'
                    )
                    
                    batch_results.append(analysis_result)
                    processed_count += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    if len(batch_results) >= batch_size:
                        db.add_all(batch_results)
                        db.commit()
                        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ø–∞–∫–µ—Ç –∏–∑ {len(batch_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
                        batch_results.clear()  # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {e}")
                    error_count += 1
                    continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if batch_results:
                db.add_all(batch_results)
                db.commit()
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–∫–µ—Ç –∏–∑ {len(batch_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
            logger.info(f"‚úÖ –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω: {processed_count} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, {error_count} –æ—à–∏–±–æ–∫")
            
            return {
                'status': 'completed',
                'processed_matches': processed_count,
                'error_matches': error_count,
                'total_found': len(documents)
            }
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'processed_matches': 0,
            'error_matches': 0
        }


@celery_app.task(
    bind=True,
    name='common.tasks.reranking_tasks.rerank_jobs_for_resume',
    soft_time_limit=600,
    time_limit=720,
    max_retries=3
)
def rerank_jobs_for_resume(self, submission_id: str, top_k: int = 50) -> Dict[str, Any]:
    """
    Task 4B: –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Ç–æ–ø-50 –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ
    
    –ù–∞—Ö–æ–¥–∏—Ç —Ç–æ–ø-50 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π —á–µ—Ä–µ–∑ ChromaDB,
    –∑–∞—Ç–µ–º –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –∏—Ö —Å –ø–æ–º–æ—â—å—é BGE Reranker –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ PostgreSQL.
    
    Args:
        submission_id: ID —Ä–µ–∑—é–º–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)
    
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
    """
    logger.info(f"üîç –ó–∞–ø—É—Å–∫ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ–∑—é–º–µ {submission_id}")

    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –µ—Å–ª–∏ submission_id –Ω–µ —Å—Ç—Ä–æ–∫–∞ (UUID), –∞ —Å–ø–∏—Å–æ–∫ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    if not isinstance(submission_id, str):
        logger.warning(f"‚ö†Ô∏è submission_id –ø–µ—Ä–µ–¥–∞–Ω –≤ –Ω–µ–≤–µ—Ä–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: {type(submission_id)}. –ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–∞—á–∏.")
        return {
            'status': 'skipped',
            'error': 'submission_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π UUID',
            'processed_matches': 0,
            'error_matches': 0
        }
    
    try:
        db = database.get_session()
        processed_count = 0
        error_count = 0
        
        try:
            # 1. –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—é–º–µ
            submission_uuid = UUID(submission_id) if isinstance(submission_id, str) else submission_id
            submission = submission_crud.get_by_id(db, submission_uuid)
            if not submission:
                logger.error(f"‚ùå –†–µ–∑—é–º–µ {submission_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                return {
                    'status': 'error',
                    'error': f'–†–µ–∑—é–º–µ {submission_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            
            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ
            if not getattr(submission, 'resume_raw_text', None) or not str(submission.resume_raw_text).strip():
                logger.error(f"‚ùå –£ —Ä–µ–∑—é–º–µ {submission_id} –Ω–µ—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
                return {
                    'status': 'error',
                    'error': f'–£ —Ä–µ–∑—é–º–µ {submission_id} –Ω–µ—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            
            # 3. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ (–æ–±—Ä–µ–∑–∞–µ–º –¥–æ 32,000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞)
            resume_text_full = str(submission.resume_raw_text)
            original_length = len(resume_text_full)
            
            if original_length > 32000:
                resume_text = resume_text_full[:32000]
                logger.info(f"üìè –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ {submission_id} –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞ —Å {original_length} –¥–æ {len(resume_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                resume_text = resume_text_full
                logger.info(f"üìè –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ {submission_id}: {original_length} —Å–∏–º–≤–æ–ª–æ–≤ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞)")
            
            # 4. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ ChromaDB
            logger.info(f"üîç –ü–æ–∏—Å–∫ —Ç–æ–ø-{top_k} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ ChromaDB")
            job_collection = chroma_client.get_job_collection()
            
            if job_collection.count() == 0:
                logger.warning("‚ö†Ô∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è –≤–∞–∫–∞–Ω—Å–∏–π –ø—É—Å—Ç–∞")
                return {
                    'status': 'completed',
                    'processed_matches': 0,
                    'error_matches': 0,
                    'total_found': 0
                }
            
            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π
            search_results = job_collection.query(
                query_texts=[resume_text],
                n_results=min(top_k, job_collection.count()),
                include=['documents', 'metadatas', 'distances']
            )
            
            if not search_results['documents'] or not search_results['documents'][0]:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π")
                return {
                    'status': 'completed',
                    'processed_matches': 0,
                    'error_matches': 0,
                    'total_found': 0
                }
            
            # 5. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
            documents = search_results['documents'][0]
            metadatas = search_results['metadatas'][0]
            distances = search_results['distances'][0]

            # –ü—Ä–æ–≤–µ—Ä–∫–∞: distances –∏ metadatas –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–∞–º–∏
            if not isinstance(distances, list):
                logger.error(f"[FATAL] distances –Ω–µ —Å–ø–∏—Å–æ–∫: {type(distances)}, value={distances}")
                return {
                    'status': 'error',
                    'error': 'distances –Ω–µ —Å–ø–∏—Å–æ–∫',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            if not isinstance(metadatas, list):
                logger.error(f"[FATAL] metadatas –Ω–µ —Å–ø–∏—Å–æ–∫: {type(metadatas)}, value={metadatas}")
                return {
                    'status': 'error',
                    'error': 'metadatas –Ω–µ —Å–ø–∏—Å–æ–∫',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(documents)} –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
            # 6. –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é BGE Reranker
            reranker = get_reranker_client()
            
            if not reranker.health_check():
                logger.error("‚ùå BGE Reranker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return {
                    'status': 'error',
                    'error': 'BGE Reranker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
                    'processed_matches': 0,
                    'error_matches': 0
                }
            
            logger.info(f"üß† –ù–∞—á–∏–Ω–∞–µ–º —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ {len(documents)} –≤–∞–∫–∞–Ω—Å–∏–π")
            reranked_results = reranker.rerank_texts(resume_text, documents)
            
            if not reranked_results:
                logger.warning("‚ö†Ô∏è –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return {
                    'status': 'completed',
                    'processed_matches': 0,
                    'error_matches': 0,
                    'total_found': len(documents)
                }
            
            # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ PostgreSQL –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(reranked_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
            batch_size = 10  # –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            batch_results = []
            
            for rank_position, (doc_idx, rerank_score) in enumerate(reranked_results, 1):
                try:
                    # –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤ –∏ –Ω–µ–≤–µ—Ä–Ω—ã—Ö —Ç–∏–ø–æ–≤
                    logger.error(f"[CHECK] doc_idx={doc_idx} ({type(doc_idx)}), distances type={type(distances)}, metadatas type={type(metadatas)}")
                    if isinstance(distances, list) and len(distances) > 0:
                        logger.error(f"[CHECK] distances[0] type={type(distances[0])}, value={distances[0]}")
                    if isinstance(metadatas, list) and len(metadatas) > 0:
                        logger.error(f"[CHECK] metadatas[0] type={type(metadatas[0])}, value={metadatas[0]}")
                    if not isinstance(doc_idx, int):
                        logger.error(f"[FATAL] doc_idx –Ω–µ int: {type(doc_idx)}, value={doc_idx}")
                        error_count += 1
                        continue
                    if doc_idx >= len(metadatas):
                        logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {doc_idx} –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
                        continue
                    if doc_idx >= len(distances):
                        logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {doc_idx} –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã distances")
                        continue
                    if isinstance(distances[doc_idx], list):
                        logger.error(f"[FATAL] distances[{doc_idx}] –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫: {distances[doc_idx]}")
                        error_count += 1
                        continue
                    
                    metadata = metadatas[doc_idx]
                    job_id = metadata.get('source_id')
                    
                    if not job_id:
                        logger.warning(f"‚ö†Ô∏è –ù–µ—Ç source_id –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {doc_idx}")
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏—é
                    job = job_crud.get_by_id(db, job_id)
                    if not job:
                        logger.warning(f"‚ö†Ô∏è –í–∞–∫–∞–Ω—Å–∏—è {job_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
                    original_distance = distances[doc_idx] if doc_idx < len(distances) else 0.0
                    original_similarity = max(0.0, 1.0 - original_distance)
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º rerank_score –æ—Ç [-10, +10] –∫ [0, 1]
                    normalized_rerank_score = max(0.0, min(1.0, (rerank_score + 10.0) / 20.0))
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è original similarity –∏ normalized rerank score)
                    final_score = (original_similarity * 0.3) + (normalized_rerank_score * 0.7)
                    score_improvement = normalized_rerank_score - original_similarity
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
                    search_params = {
                        'top_k': top_k,
                        'min_similarity': 0.0,
                        'min_rerank_score': -10.0,
                        'search_type': 'resume_to_jobs',
                        'query_text_length': len(resume_text),
                        'original_text_length': original_length,
                        'text_truncated': original_length > 32000
                    }
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ workflow
                    workflow_stats = {
                        'total_candidates_found': len(documents),
                        'reranked_candidates': len(reranked_results),
                        'processing_time': datetime.utcnow().isoformat(),
                        'chroma_collection': ChromaConfig.JOB_COLLECTION
                    }
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –∞–Ω–∞–ª–∏–∑–∞
                    analysis_result = RerankerAnalysisResult(
                        job_id=job_id,
                        submission_id=submission.submission_id,
                        original_similarity=original_similarity,
                        rerank_score=rerank_score,
                        final_score=final_score,
                        score_improvement=score_improvement,
                        rank_position=rank_position,
                        search_params=search_params,
                        reranker_model=reranker.model_name,
                        workflow_stats=workflow_stats,
                        job_title=job.title or 'Unknown',
                        company_id=job.company_id,
                        candidate_name=f"{submission.candidate.first_name or ''} {submission.candidate.last_name or ''}".strip() or 'Unknown',
                        candidate_email=submission.candidate.email or 'Unknown',
                        total_candidates_found=len(documents),
                        analysis_type='resume_to_jobs_rerank'
                    )
                    
                    batch_results.append(analysis_result)
                    processed_count += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    if len(batch_results) >= batch_size:
                        db.add_all(batch_results)
                        db.commit()
                        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ø–∞–∫–µ—Ç –∏–∑ {len(batch_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
                        batch_results.clear()  # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
                    
                except Exception as e:
                    import traceback
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {e}\nTRACEBACK:\n{traceback.format_exc()}")
                    logger.error(f"[EXCEPT-DIAG] doc_idx={locals().get('doc_idx', None)} ({type(locals().get('doc_idx', None))}), distances type={type(locals().get('distances', None))}, metadatas type={type(locals().get('metadatas', None))}")
                    try:
                        logger.error(f"[EXCEPT-DIAG] distances={locals().get('distances', None)}")
                        logger.error(f"[EXCEPT-DIAG] metadatas={locals().get('metadatas', None)}")
                    except Exception as diag_e:
                        logger.error(f"[EXCEPT-DIAG] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ distances/metadatas: {diag_e}")
                    error_count += 1
                    continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if batch_results:
                db.add_all(batch_results)
                db.commit()
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–∫–µ—Ç –∏–∑ {len(batch_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
            logger.info(f"‚úÖ –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ–∑—é–º–µ {submission_id} –∑–∞–≤–µ—Ä—à–µ–Ω: {processed_count} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, {error_count} –æ—à–∏–±–æ–∫")
            
            return {
                'status': 'completed',
                'processed_matches': processed_count,
                'error_matches': error_count,
                'total_found': len(documents)
            }
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ–∑—é–º–µ {submission_id}: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'processed_matches': 0,
            'error_matches': 0
        }


# –û—Ç–∫–ª—é—á–∞–µ–º –∑–∞–¥–∞—á—É process_all_reranking (–≤—Ä–µ–º–µ–Ω–Ω–æ)
# @celery_app.task(
#     bind=True,
#     name='tasks.reranking_tasks.process_all_reranking',
#     soft_time_limit=1800,
#     time_limit=2400,
#     max_retries=2
# )
# def process_all_reranking(self, top_k: int = 50) -> Dict[str, Any]:
#     """
#     Task 4: –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–∏
    
#     –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
#     –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤—ã—Ö —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–π.
    
#     Args:
#         top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)
    
#     Returns:
#         Dict —Å –æ–±—â–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
#     """
#     logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ (—Ç–æ–ø-{top_k})")
    
#     try:
#         db = database.get_session()
#         total_resume_matches = 0
#         total_job_matches = 0
#         processed_resumes = 0
#         processed_jobs = 0
#         error_resumes = 0
#         error_jobs = 0
        
#         try:
#             # 1. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ä–µ–∑—é–º–µ —Å —Ç–µ–∫—Å—Ç–æ–º, –Ω–æ –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
#             resumes_for_reranking = db.query(Submission).filter(
#                 Submission.resume_raw_text.isnot(None),
#                 Submission.resume_raw_text != '',
#                 ~Submission.submission_id.in_(
#                     db.query(RerankerAnalysisResult.submission_id).filter(
#                         RerankerAnalysisResult.analysis_type == 'resume_to_jobs_rerank'
#                     )
#                 )
#             ).all()
            
#             logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(resumes_for_reranking)} —Ä–µ–∑—é–º–µ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
#             # 2. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Ç–µ–∫—Å—Ç–æ–º, –Ω–æ –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
#             jobs_for_reranking = db.query(Job).filter(
#                 Job.job_description_raw_text.isnot(None),
#                 Job.job_description_raw_text != '',
#                 ~Job.job_id.in_(
#                     db.query(RerankerAnalysisResult.job_id).filter(
#                         RerankerAnalysisResult.analysis_type == 'job_to_resumes_rerank'
#                     )
#                 )
#             ).all()
            
#             logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(jobs_for_reranking)} –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
#             # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—é–º–µ (—Ü–µ–ø–æ—á–∫–∞ B: —Ä–µ–∑—é–º–µ -> —Ç–æ–ø-50 –≤–∞–∫–∞–Ω—Å–∏–π)
#             for submission in resumes_for_reranking:
#                 try:
#                     logger.info(f"üîÑ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–¥–∞—á–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è —Ä–µ–∑—é–º–µ {submission.submission_id}")
                    
#                     # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è —Ä–µ–∑—é–º–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (–ë–ï–ó .get()!)
#                     task_result = rerank_jobs_for_resume.apply_async(
#                         args=[str(submission.submission_id), top_k],
#                         queue=RERANKING_QUEUE
#                     )
                    
#                     # –ù–ï –≤—ã–∑—ã–≤–∞–µ–º .get() - –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏
#                     logger.info(f"üì§ –ó–∞–¥–∞—á–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è —Ä–µ–∑—é–º–µ {submission.submission_id} –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞: {task_result.id}")
#                     processed_resumes += 1
                    
#                 except Exception as e:
#                     logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–µ–∑—é–º–µ {submission.submission_id}: {e}")
#                     error_resumes += 1
#                     continue
            
#             # 4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ (—Ü–µ–ø–æ—á–∫–∞ A: –≤–∞–∫–∞–Ω—Å–∏—è -> —Ç–æ–ø-50 —Ä–µ–∑—é–º–µ)
#             for job in jobs_for_reranking:
#                 try:
#                     logger.info(f"üîÑ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–¥–∞—á–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job.job_id}")
                    
#                     # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (–ë–ï–ó .get()!)
#                     task_result = rerank_resumes_for_job.apply_async(
#                         args=[job.job_id, top_k],
#                         queue=RERANKING_QUEUE
#                     )
                    
#                     # –ù–ï –≤—ã–∑—ã–≤–∞–µ–º .get() - –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏
#                     logger.info(f"üì§ –ó–∞–¥–∞—á–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job.job_id} –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞: {task_result.id}")
#                     processed_jobs += 1
                    
#                 except Exception as e:
#                     logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–¥–∞—á–∏ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job.job_id}: {e}")
#                     error_jobs += 1
#                     continue
            
#             logger.info(f"‚úÖ –í—Å–µ –∑–∞–¥–∞—á–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã: {processed_resumes} —Ä–µ–∑—é–º–µ, {processed_jobs} –≤–∞–∫–∞–Ω—Å–∏–π")
            
#             return {
#                 'status': 'completed',
#                 'total_resumes_submitted': processed_resumes,
#                 'total_jobs_submitted': processed_jobs,
#                 'error_resumes': error_resumes,
#                 'error_jobs': error_jobs,
#                 'total_found_resumes': len(resumes_for_reranking),
#                 'total_found_jobs': len(jobs_for_reranking)
#             }
            
#         finally:
#             db.close()
            
#     except Exception as e:
#         logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {e}")
#         return {
#             'status': 'error',
#             'error': str(e),
#             'total_resumes_submitted': 0,
#             'total_jobs_submitted': 0
#         }
