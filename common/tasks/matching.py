"""
–ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Celery –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–π

–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
- –ü–æ–∏—Å–∫ —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ (find_matching_resumes_for_job)
- –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ–∑—é–º–µ (find_matching_jobs_for_resume)
- –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (batch_find_matches_for_jobs, batch_find_matches_for_resumes)
- –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è (_simple –≤–∞—Ä–∏–∞–Ω—Ç—ã)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã –∏–∑ tasks.base –¥–ª—è retry, —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
"""

from datetime import datetime
from typing import Dict, List, Optional, Any
from celery.utils.log import get_task_logger
from database.operations.candidate_operations import SubmissionCRUD
from database.operations.company_operations import JobCRUD
from utils.chroma_config import chroma_client, ChromaConfig
from tasks.task_utils import safe_uuid_convert, serialize_for_json, mask_sensitive_data
from tasks.base import get_db_session, safe_task, monitored_task, standard_retry_policy, celery_friendly_delay

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Celery app
from celery_app.celery_app import get_celery_app

app = get_celery_app()

logger = get_task_logger(__name__)


@app.task(
    bind=True, 
    name='tasks.matching.find_matching_resumes_for_job',
    soft_time_limit=300,  # 5 –º–∏–Ω—É—Ç
    time_limit=360        # 6 –º–∏–Ω—É—Ç
)
@safe_task
@monitored_task
@standard_retry_policy(max_retries=3, countdown=30)
def find_matching_resumes_for_job(self, job_id: int, top_k: int = 20, min_similarity: float = 0.4) -> Dict[str, Any]:
    """
    –û–°–ù–û–í–ù–ê–Ø –ó–ê–î–ê–ß–ê: –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏
    
    Args:
        job_id: ID –≤–∞–∫–∞–Ω—Å–∏–∏
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Ä–µ–∑—é–º–µ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20)
        min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.4)
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ —Ä–µ–π—Ç–∏–Ω–≥–æ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    """
    logger.info(f"üéØ –ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
    
    with get_db_session() as db:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ChromaDB
        if not chroma_client.health_check():
            logger.error("‚ùå ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            raise Exception("ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º job_id
        if not isinstance(job_id, int) or job_id <= 0:
            logger.error(f"‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π job_id: {job_id}")
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π job_id: {job_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏—é
        job = JobCRUD.get_by_id(db, job_id)
        if not job:
            logger.error(f"‚ùå –í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {job_id}")
            raise ValueError(f"–í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {job_id}")
        
        if not getattr(job, 'job_description_raw_text', None):
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
            return {
                'job_id': job_id,
                'job_title': job.title,
                'matches': [],
                'total_found': 0,
                'message': '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏'
            }
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ChromaDB
        resume_collection = chroma_client.get_or_create_collection(ChromaConfig.RESUME_COLLECTION)
        job_collection = chroma_client.get_or_create_collection(ChromaConfig.JOB_COLLECTION)
        
        # –ò—â–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –¥–∞–Ω–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
        job_prefix = f"job_{job_id}"
        all_job_results = job_collection.get(include=['embeddings', 'metadatas'])
        
        matching_job_id = None
        job_embedding = None
        for idx, job_id_str in enumerate(all_job_results['ids']):
            if job_id_str.startswith(job_prefix):
                matching_job_id = job_id_str
                job_embedding = all_job_results['embeddings'][idx]
                break
        
        if not matching_job_id:
            logger.warning(f"‚ö†Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ ChromaDB")
            return {
                'job_id': job_id,
                'job_title': job.title,
                'matches': [],
                'total_found': 0,
                'message': '–≠–º–±–µ–¥–¥–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥.'
            }
        
        # –ò—â–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ —Ä–µ–∑—é–º–µ
        logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∑—é–º–µ –≤ ChromaDB (top_{top_k})")
        resume_matches = resume_collection.query(
            query_embeddings=[job_embedding],
            n_results=min(top_k * 2, 100),  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            include=['distances', 'metadatas', 'documents']
        )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        matches = []
        if resume_matches['ids'] and resume_matches['ids'][0]:
            submission_ids = resume_matches['ids'][0]
            distances = resume_matches['distances'][0]
            metadatas = resume_matches['metadatas'][0]
            documents = resume_matches['documents'][0]
            
            logger.info(f"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(submission_ids)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ ChromaDB")
            
            for i, submission_id_str in enumerate(submission_ids):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ —Å—Ö–æ–¥—Å—Ç–≤–æ (cosine similarity)
                similarity = 1.0 - distances[i]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –ø–æ—Ä–æ–≥—É —Å—Ö–æ–¥—Å—Ç–≤–∞
                if similarity >= min_similarity:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º UUID –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞ 'resume_{UUID}_{hash}'
                    submission_uuid = None
                    if submission_id_str.startswith('resume_'):
                        parts = submission_id_str.split('_')
                        if len(parts) >= 2:
                            submission_uuid = safe_uuid_convert(parts[1])
                    
                    if not submission_uuid:
                        continue
                    
                    match_data = {
                        'submission_id': str(submission_uuid),
                        'similarity': round(similarity, 4),
                        'distance': round(distances[i], 4),
                        'chroma_id': submission_id_str,
                        'metadata': metadatas[i] if metadatas[i] else {},
                        'snippet': documents[i][:300] + '...' if documents[i] and len(documents[i]) > 300 else documents[i]
                    }
                    matches.append(match_data)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        matches.sort(key=lambda x: x['similarity'], reverse=True)
        logger.info(f"üìà –ù–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ {min_similarity}")
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤ –ë–î –∏ –æ–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–º–∏
        validated_matches = []
        for match in matches[:top_k]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ top_k
            submission_uuid = safe_uuid_convert(match['submission_id'])
            if submission_uuid:
                submission = SubmissionCRUD.get_by_id(db, submission_uuid)
                if submission:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–¥–∏–¥–∞—Ç–µ
                    match['candidate_id'] = submission.candidate_id
                    match['submission_status'] = getattr(submission, 'status', 'unknown')
                    
                    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞–Ω–¥–∏–¥–∞—Ç–µ (–µ—Å–ª–∏ —Å–≤—è–∑—å –µ—Å—Ç—å)
                    if hasattr(submission, 'candidate') and submission.candidate:
                        candidate = submission.candidate
                        match['candidate_name'] = f"{candidate.first_name} {candidate.last_name}"
                        match['candidate_email'] = mask_sensitive_data(getattr(candidate, 'email', ''))
                    else:
                        match['candidate_name'] = 'Unknown'
                        match['candidate_email'] = 'Unknown'
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∑—é–º–µ
                    if hasattr(submission, 'resume_url'):
                        match['resume_url'] = getattr(submission, 'resume_url', None)
                    
                    validated_matches.append(match)
        
        logger.info(f"‚úÖ –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–æ {len(validated_matches)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ '{job.title}'")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'job_id': job_id,
            'job_title': job.title,
            'company_id': job.company_id,
            'matches': validated_matches,
            'total_found': len(validated_matches),
            'search_params': {
                'top_k': top_k,
                'min_similarity': min_similarity,
                'requested_top_k': top_k
            },
            'statistics': {
                'chroma_results': len(submission_ids) if resume_matches['ids'] and resume_matches['ids'][0] else 0,
                'above_threshold': len(matches),
                'validated_in_db': len(validated_matches)
            },
            'processed_at': datetime.utcnow().isoformat(),
            'message': f'–ù–∞–π–¥–µ–Ω–æ {len(validated_matches)} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤'
        }
        
        return result


@app.task(
    bind=True, 
    name='tasks.matching.find_matching_jobs_for_resume',
    soft_time_limit=300,  # 5 –º–∏–Ω—É—Ç
    time_limit=360        # 6 –º–∏–Ω—É—Ç
)
@safe_task
@monitored_task
@standard_retry_policy(max_retries=3, countdown=30)
def find_matching_jobs_for_resume(self, submission_id: str, top_k: int = 10, min_similarity: float = 0.3) -> Dict[str, Any]:
    """
    –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ó–ê–î–ê–ß–ê: –ü–æ–∏—Å–∫ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ–∑—é–º–µ
    
    Args:
        submission_id: ID –∑–∞—è–≤–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-–≤–∞–∫–∞–Ω—Å–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)
        min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.3)
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    """
    logger.info(f"üîç –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ–∑—é–º–µ {mask_sensitive_data(submission_id)}")
    
    with get_db_session() as db:
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º submission_id
        submission_uuid = safe_uuid_convert(submission_id)
        if not submission_uuid:
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π submission_id: {submission_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞—è–≤–∫—É –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        submission = SubmissionCRUD.get_by_id(db, submission_uuid)
        if not submission:
            raise ValueError(f"–ó–∞—è–≤–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {submission_id}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        resume_collection = chroma_client.get_or_create_collection(ChromaConfig.RESUME_COLLECTION)
        job_collection = chroma_client.get_or_create_collection(ChromaConfig.JOB_COLLECTION)
        
        resume_prefix = f"resume_{submission_uuid}"
        all_resume_results = resume_collection.get(include=['embeddings'])
        
        resume_embedding = None
        for idx, resume_id in enumerate(all_resume_results['ids']):
            if resume_id.startswith(resume_prefix):
                resume_embedding = all_resume_results['embeddings'][idx]
                break
        
        if resume_embedding is None or len(resume_embedding) == 0:
            return {
                'submission_id': submission_id,
                'matches': [],
                'total_found': 0,
                'message': '–≠–º–±–µ–¥–¥–∏–Ω–≥ —Ä–µ–∑—é–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω'
            }
        
        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π
        job_matches = job_collection.query(
            query_embeddings=[resume_embedding],
            n_results=top_k,
            include=['distances', 'metadatas', 'documents']
        )
        
        matches = []
        if job_matches['ids'] and job_matches['ids'][0]:
            for i, job_id_str in enumerate(job_matches['ids'][0]):
                similarity = 1.0 - job_matches['distances'][0][i]
                
                if similarity >= min_similarity:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º job_id
                    try:
                        if job_id_str.startswith('job_'):
                            parts = job_id_str.split('_')
                            job_id_num = int(parts[1]) if len(parts) >= 2 else None
                        else:
                            job_id_num = int(job_id_str)
                    except (ValueError, IndexError):
                        continue
                    
                    if job_id_num:
                        job = JobCRUD.get_by_id(db, job_id_num)
                        if job and getattr(job, 'is_active', True):
                            matches.append({
                                'job_id': job_id_num,
                                'job_title': job.title,
                                'company_id': job.company_id,
                                'similarity': round(similarity, 4),
                                'distance': round(job_matches['distances'][0][i], 4)
                            })
        
        return {
            'submission_id': submission_id,
            'candidate_id': submission.candidate_id,
            'matches': sorted(matches, key=lambda x: x['similarity'], reverse=True),
            'total_found': len(matches),
            'processed_at': datetime.utcnow().isoformat()
        }


# ================== BATCH OPERATIONS ==================

@app.task(
    bind=True, 
    name='tasks.matching.batch_find_matches_for_resumes',
    soft_time_limit=600,  # 10 –º–∏–Ω—É—Ç –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    time_limit=720        # 12 –º–∏–Ω—É—Ç
)
@safe_task
@monitored_task
@standard_retry_policy(max_retries=3, countdown=120)
def batch_find_matches_for_resumes(self, submission_ids: List[str], top_k: int = 50, min_similarity: float = 0.5) -> Dict[str, Any]:
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ä–µ–∑—é–º–µ
    
    Args:
        submission_ids: –°–ø–∏—Å–æ–∫ ID –∑–∞—è–≤–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-–≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—é–º–µ
        min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    """
    logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è {len(submission_ids)} —Ä–µ–∑—é–º–µ")
    
    total_submissions = len(submission_ids)
    processed_count = 0
    successful_matches = 0
    failed_matches = 0
    results = {}
    
    for i, submission_id in enumerate(submission_ids):
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            progress = int((i / total_submissions) * 100)
            self.update_state(
                state='PROGRESS',
                meta={
                    'progress': progress,
                    'status': f'–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—é–º–µ {i+1} –∏–∑ {total_submissions}',
                    'processed': processed_count,
                    'successful': successful_matches,
                    'failed': failed_matches
                }
            )
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
            if i > 0:
                import time
                time.sleep(0.1)
            
            # –í—ã–∑—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É –ø–æ–∏—Å–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ
            result = find_matching_jobs_for_resume.apply(
                args=[submission_id, top_k, min_similarity]
            ).get()
            
            results[submission_id] = result
            successful_matches += 1
            
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–µ–∑—é–º–µ {submission_id}: –Ω–∞–π–¥–µ–Ω–æ {result.get('total_found', 0)} –≤–∞–∫–∞–Ω—Å–∏–π")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—é–º–µ {submission_id}: {str(e)}")
            results[submission_id] = {
                'submission_id': submission_id,
                'error': str(e),
                'matches': [],
                'total_found': 0
            }
            failed_matches += 1
        
        processed_count += 1
    
    logger.info(f"‚úÖ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_matches} —É—Å–ø–µ—à–Ω—ã—Ö, {failed_matches} —Å –æ—à–∏–±–∫–∞–º–∏")
    
    summary = {
        'total_processed': processed_count,
        'successful_matches': successful_matches,
        'failed_matches': failed_matches,
        'results': results,
        'search_params': {
            'top_k': top_k,
            'min_similarity': min_similarity
        },
        'processed_at': datetime.utcnow().isoformat()
    }
    
    return summary


@app.task(
    bind=True, 
    name='tasks.matching.batch_find_matches_for_jobs',
    soft_time_limit=600,  # 10 –º–∏–Ω—É—Ç –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    time_limit=720        # 12 –º–∏–Ω—É—Ç
)
@safe_task
@monitored_task
@standard_retry_policy(max_retries=3, countdown=120)
def batch_find_matches_for_jobs(self, job_ids: List[int], top_k: int = 50, min_similarity: float = 0.5) -> Dict[str, Any]:
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–∑—é–º–µ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –≤–∞–∫–∞–Ω—Å–∏–π
    
    Args:
        job_ids: –°–ø–∏—Å–æ–∫ ID –≤–∞–∫–∞–Ω—Å–∏–π
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Ä–µ–∑—é–º–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
        min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    """
    logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–∑—é–º–µ –¥–ª—è {len(job_ids)} –≤–∞–∫–∞–Ω—Å–∏–π")
    
    total_jobs = len(job_ids)
    processed_count = 0
    successful_matches = 0
    failed_matches = 0
    results = {}
    
    for i, job_id in enumerate(job_ids):
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            progress = int((i / total_jobs) * 100)
            self.update_state(
                state='PROGRESS',
                meta={
                    'progress': progress,
                    'status': f'–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ {i+1} –∏–∑ {total_jobs}',
                    'processed': processed_count,
                    'successful': successful_matches,
                    'failed': failed_matches
                }
            )
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
            if i > 0:
                import time
                time.sleep(0.1)
            
            # –í—ã–∑—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É –ø–æ–∏—Å–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
            result = find_matching_resumes_for_job.apply(
                args=[job_id, top_k, min_similarity]
            ).get()
            
            results[str(job_id)] = result
            successful_matches += 1
            
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è {job_id}: –Ω–∞–π–¥–µ–Ω–æ {result.get('total_found', 0)} —Ä–µ–∑—é–º–µ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}: {str(e)}")
            results[str(job_id)] = {
                'job_id': job_id,
                'error': str(e),
                'matches': [],
                'total_found': 0
            }
            failed_matches += 1
        
        processed_count += 1
    
    logger.info(f"‚úÖ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_matches} —É—Å–ø–µ—à–Ω—ã—Ö, {failed_matches} —Å –æ—à–∏–±–∫–∞–º–∏")
    
    summary = {
        'total_processed': processed_count,
        'successful_matches': successful_matches,
        'failed_matches': failed_matches,
        'results': results,
        'search_params': {
            'top_k': top_k,
            'min_similarity': min_similarity
        },
        'processed_at': datetime.utcnow().isoformat()
    }
    
    return summary


# ================== SIMPLE VERSIONS (–±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è) ==================

@app.task(
    bind=True, 
    name='tasks.matching.find_matching_jobs_for_resume_simple',
    soft_time_limit=180,  # 3 –º–∏–Ω—É—Ç—ã –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á
    time_limit=240        # 4 –º–∏–Ω—É—Ç—ã
)
@safe_task
@monitored_task
@standard_retry_policy(max_retries=2, countdown=20)
def find_matching_jobs_for_resume_simple(self, submission_id: str, top_k: int = 100, min_similarity: float = 0.5) -> Dict[str, Any]:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ–∑—é–º–µ (–±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π —Å–æ—Å—Ç–æ—è–Ω–∏—è)
    
    –ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.
    """
    logger.info(f"üîç –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Ä–µ–∑—é–º–µ {mask_sensitive_data(submission_id)}")
    
    with get_db_session() as db:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ChromaDB
        if not chroma_client.health_check():
            logger.error("‚ùå ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            raise Exception("ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º submission_id
        submission_uuid = safe_uuid_convert(submission_id)
        if not submission_uuid:
            logger.error(f"‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π submission_id: {submission_id}")
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π submission_id: {submission_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞—è–≤–∫—É –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        submission = SubmissionCRUD.get_by_id(db, submission_uuid)
        
        if not submission or not getattr(submission, 'resume_raw_text', None):
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ –¥–ª—è –∑–∞—è–≤–∫–∏ {submission_id}")
            return {
                'submission_id': submission_id,
                'matches': [],
                'total_found': 0,
                'message': '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ'
            }
        
        # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –≤ ChromaDB
        resume_collection = chroma_client.get_or_create_collection(ChromaConfig.RESUME_COLLECTION)
        job_collection = chroma_client.get_or_create_collection(ChromaConfig.JOB_COLLECTION)
        
        resume_prefix = f"resume_{submission_uuid}"
        all_resume_results = resume_collection.get(include=['embeddings'])
        
        resume_embedding = None
        for idx, resume_id in enumerate(all_resume_results['ids']):
            if resume_id.startswith(resume_prefix):
                resume_embedding = all_resume_results['embeddings'][idx]
                break
        
        if not resume_embedding:
            return {
                'submission_id': submission_id,
                'matches': [],
                'total_found': 0,
                'message': '–≠–º–±–µ–¥–¥–∏–Ω–≥ —Ä–µ–∑—é–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω'
            }
        
        # –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π
        job_matches = job_collection.query(
            query_embeddings=[resume_embedding],
            n_results=top_k,
            include=['distances', 'metadatas']
        )
        
        matches = []
        if job_matches['ids'] and job_matches['ids'][0]:
            for i, job_id_str in enumerate(job_matches['ids'][0]):
                similarity = 1.0 - job_matches['distances'][0][i]
                
                if similarity >= min_similarity:
                    try:
                        if job_id_str.startswith('job_'):
                            job_id_num = int(job_id_str.split('_')[1])
                        else:
                            job_id_num = int(job_id_str)
                        
                        matches.append({
                            'job_id': job_id_num,
                            'similarity': round(similarity, 4),
                            'distance': round(job_matches['distances'][0][i], 4)
                        })
                    except (ValueError, IndexError):
                        continue
        
        return {
            'submission_id': submission_id,
            'matches': sorted(matches, key=lambda x: x['similarity'], reverse=True),
            'total_found': len(matches),
            'processed_at': datetime.utcnow().isoformat()
        }


@app.task(
    bind=True, 
    name='tasks.matching.find_matching_resumes_for_job_simple',
    soft_time_limit=180,  # 3 –º–∏–Ω—É—Ç—ã –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á
    time_limit=240        # 4 –º–∏–Ω—É—Ç—ã
)
@safe_task
@monitored_task
@standard_retry_policy(max_retries=2, countdown=20)
def find_matching_resumes_for_job_simple(self, job_id: int, top_k: int = 50, min_similarity: float = 0.4) -> Dict[str, Any]:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ (–±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π —Å–æ—Å—Ç–æ—è–Ω–∏—è)
    
    –ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.
    """
    logger.info(f"üéØ –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
    
    with get_db_session() as db:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ChromaDB
        if not chroma_client.health_check():
            raise Exception("ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º job_id
        if not isinstance(job_id, int) or job_id <= 0:
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π job_id: {job_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏—é
        job = JobCRUD.get_by_id(db, job_id)
        if not job:
            raise ValueError(f"–í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {job_id}")
        
        # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –≤ ChromaDB
        resume_collection = chroma_client.get_or_create_collection(ChromaConfig.RESUME_COLLECTION)
        job_collection = chroma_client.get_or_create_collection(ChromaConfig.JOB_COLLECTION)
        
        job_prefix = f"job_{job_id}"
        all_job_results = job_collection.get(include=['embeddings'])
        
        job_embedding = None
        for idx, job_id_str in enumerate(all_job_results['ids']):
            if job_id_str.startswith(job_prefix):
                job_embedding = all_job_results['embeddings'][idx]
                break
        
        if not job_embedding:
            return {
                'job_id': job_id,
                'matches': [],
                'total_found': 0,
                'message': '–≠–º–±–µ–¥–¥–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω'
            }
        
        # –ü–æ–∏—Å–∫ —Ä–µ–∑—é–º–µ
        resume_matches = resume_collection.query(
            query_embeddings=[job_embedding],
            n_results=top_k,
            include=['distances', 'metadatas']
        )
        
        matches = []
        if resume_matches['ids'] and resume_matches['ids'][0]:
            for i, submission_id_str in enumerate(resume_matches['ids'][0]):
                similarity = 1.0 - resume_matches['distances'][0][i]
                
                if similarity >= min_similarity:
                    if submission_id_str.startswith('resume_'):
                        parts = submission_id_str.split('_')
                        if len(parts) >= 2:
                            submission_uuid = safe_uuid_convert(parts[1])
                            if submission_uuid:
                                matches.append({
                                    'submission_id': str(submission_uuid),
                                    'similarity': round(similarity, 4),
                                    'distance': round(resume_matches['distances'][0][i], 4)
                                })
        
        return {
            'job_id': job_id,
            'job_title': job.title,
            'matches': sorted(matches, key=lambda x: x['similarity'], reverse=True),
            'total_found': len(matches),
            'processed_at': datetime.utcnow().isoformat()
        }


# ================== ALIASES FOR BACKWARD COMPATIBILITY ==================

# –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å existing imports
batch_find_matches_for_resumes = batch_find_matches_for_resumes
batch_find_matches_for_jobs = batch_find_matches_for_jobs
find_matching_jobs_for_resume_simple = find_matching_jobs_for_resume_simple
find_matching_resumes_for_job_simple = find_matching_resumes_for_job_simple
