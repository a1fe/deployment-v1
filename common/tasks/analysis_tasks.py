"""
Celery –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ BGE Reranker

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ enhanced search –≤ PostgreSQL
"""

from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from celery.utils.log import get_task_logger
from database.operations.analysis_operations import reranker_analysis_result_crud, reranker_analysis_session_crud
from tasks.task_utils import get_db_session, serialize_for_json, mask_sensitive_data

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Celery app
from celery_app.celery_app import get_celery_app

app = get_celery_app()

logger = get_task_logger(__name__)


@app.task(
    bind=True, 
    name='tasks.analysis.save_reranker_results', 
    max_retries=3, 
    default_retry_delay=60,
    soft_time_limit=300,  # 5 –º–∏–Ω—É—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    time_limit=360        # 6 –º–∏–Ω—É—Ç
)
def save_reranker_analysis_results(
    self,
    job_id: int,
    enhanced_search_data: Dict[str, Any],
    session_metadata: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ê–ù–ê–õ–ò–ó–ê: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ enhanced search
    
    –≠—Ç–∞ –∑–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è enhanced_resume_search –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ BGE Reranker –≤ PostgreSQL –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
    
    Args:
        job_id: ID –≤–∞–∫–∞–Ω—Å–∏–∏
        enhanced_search_data: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã enhanced search –∏–∑ tasks.scoring.enhanced_resume_search
        session_metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–µ—Å—Å–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    """
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ BGE Reranker –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
    
    with get_db_session() as db:
        try:
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if not enhanced_search_data:
                logger.warning("‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ enhanced search –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                return serialize_for_json({
                    'job_id': job_id,
                    'saved_results': 0,
                    'session_created': False,
                    'message': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è'
                })
            
            enhanced_matches = enhanced_search_data.get('enhanced_matches', [])
            if not enhanced_matches:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç enhanced matches –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
                return serialize_for_json({
                    'job_id': job_id,
                    'saved_results': 0,
                    'session_created': False,
                    'message': '–ù–µ—Ç enhanced matches –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è'
                })
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            search_params = enhanced_search_data.get('search_params', {})
            workflow = enhanced_search_data.get('workflow', {})
            reranker_model = workflow.get('reranker_model', 'unknown')
            comprehensive_stats = enhanced_search_data.get('comprehensive_statistics', {})
            company_id = enhanced_search_data.get('company_id')
            
            if not company_id:
                logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç company_id –≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç company_id –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞
            session_data = session_metadata or {}
            session_started = session_data.get('started_at')
            if isinstance(session_started, str):
                session_started = datetime.fromisoformat(session_started.replace('Z', '+00:00'))
            elif not session_started:
                session_started = datetime.utcnow()
            
            analysis_session = reranker_analysis_session_crud.create_session(
                db=db,
                job_id=job_id,
                company_id=company_id,
                search_params=search_params,
                reranker_model=reranker_model,
                session_stats=comprehensive_stats,
                total_results=len(enhanced_matches),
                started_at=session_started,
                completed_at=datetime.utcnow()
            )
            
            logger.info(f"üìù –°–æ–∑–¥–∞–Ω–∞ —Å–µ—Å—Å–∏—è –∞–Ω–∞–ª–∏–∑–∞: {analysis_session.session_uuid}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
            saved_results = reranker_analysis_result_crud.create_from_enhanced_search(
                db=db,
                job_id=job_id,
                enhanced_matches=enhanced_matches,
                search_params=search_params,
                workflow_stats=comprehensive_stats,
                reranker_model=reranker_model,
                session_uuid=str(analysis_session.session_uuid)
            )
            
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(saved_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞")
            
            # –ú–∞—Å–∫–∏—Ä—É–µ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ª–æ–≥–∞—Ö
            masked_results = []
            for result in saved_results:
                masked_result = {
                    'analysis_id': result.analysis_id,
                    'job_id': result.job_id,
                    'submission_id': str(result.submission_id)[:8] + '...',
                    'candidate_email': mask_sensitive_data(str(result.candidate_email)),
                    'rerank_score': float(result.rerank_score) if result.rerank_score is not None else 0.0,
                    'rank_position': result.rank_position
                }
                masked_results.append(masked_result)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                'job_id': job_id,
                'company_id': company_id,
                'session_uuid': str(analysis_session.session_uuid),
                'saved_results': len(saved_results),
                'session_created': True,
                'reranker_model': reranker_model,
                'analysis_details': {
                    'total_enhanced_matches': len(enhanced_matches),
                    'search_params': search_params,
                    'workflow_completed': workflow.get('step2_reranking') == 'completed',
                    'session_started': session_started.isoformat(),
                    'session_completed': analysis_session.completed_at.isoformat()
                },
                'saved_analysis_ids': [r.analysis_id for r in saved_results],
                'sample_results': masked_results[:5],  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                'processed_at': datetime.utcnow().isoformat(),
                'message': f'–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(saved_results)} –∑–∞–ø–∏—Å–µ–π'
            }
            
            logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ: —Å–µ—Å—Å–∏—è {analysis_session.session_uuid}, {len(saved_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return serialize_for_json(result)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}: {str(e)}")
            # Retry –ª–æ–≥–∏–∫–∞
            if self.request.retries < self.max_retries:
                logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {self.request.retries + 1}/{self.max_retries}")
                raise self.retry(countdown=60 * (self.request.retries + 1))
            raise


@app.task(
    bind=True, 
    name='tasks.analysis.get_analysis_summary', 
    max_retries=2, 
    default_retry_delay=30,
    soft_time_limit=120,  # 2 –º–∏–Ω—É—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏
    time_limit=180        # 3 –º–∏–Ω—É—Ç—ã
)
def get_reranker_analysis_summary(self, job_id: int, limit: int = 20) -> Dict[str, Any]:
    """
    –ü–û–õ–£–ß–ï–ù–ò–ï –°–í–û–î–ö–ò –ê–ù–ê–õ–ò–ó–ê: –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏
    
    Args:
        job_id: ID –≤–∞–∫–∞–Ω—Å–∏–∏
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20)
        
    Returns:
        –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
    """
    logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
    
    with get_db_session() as db:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            latest_results = reranker_analysis_result_crud.get_latest_by_job(
                db=db,
                job_id=job_id,
                limit=limit
            )
            
            if not latest_results:
                return serialize_for_json({
                    'job_id': job_id,
                    'has_analysis': False,
                    'total_results': 0,
                    'message': '–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏'
                })
            
            # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫—É
            analytics = reranker_analysis_result_crud.get_analytics_by_job(db=db, job_id=job_id)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–µ—Å—Å–∏–∏
            recent_sessions = reranker_analysis_session_crud.get_recent_sessions(
                db=db,
                job_id=job_id,
                limit=5
            )
            
            # –ú–∞—Å–∫–∏—Ä—É–µ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            masked_results = []
            for result in latest_results:
                summary = result.to_summary_dict()
                summary['candidate_email'] = mask_sensitive_data(summary['candidate_email'])
                summary['submission_id'] = summary['submission_id'][:8] + '...'
                masked_results.append(summary)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É
            summary = {
                'job_id': job_id,
                'has_analysis': True,
                'total_results': len(latest_results),
                'latest_results': masked_results,
                'analytics': analytics,
                'recent_sessions': [
                    {
                        'session_uuid': str(s.session_uuid)[:8] + '...',
                        'total_results': s.total_results,
                        'reranker_model': s.reranker_model,
                        'completed_at': s.completed_at.isoformat() if hasattr(s.completed_at, 'isoformat') else None
                    } for s in recent_sessions
                ],
                'retrieved_at': datetime.utcnow().isoformat()
            }
            
            logger.info(f"üìã –ü–æ–ª—É—á–µ–Ω–∞ —Å–≤–æ–¥–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {len(latest_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return serialize_for_json(summary)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–≤–æ–¥–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}: {str(e)}")
            # Retry –ª–æ–≥–∏–∫–∞
            if self.request.retries < self.max_retries:
                logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {self.request.retries + 1}/{self.max_retries}")
                raise self.retry(countdown=30)
            raise


@app.task(
    bind=True, 
    name='tasks.analysis.cleanup_old_analysis', 
    max_retries=2, 
    default_retry_delay=60,
    soft_time_limit=600,  # 10 –º–∏–Ω—É—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
    time_limit=720        # 12 –º–∏–Ω—É—Ç
)
def cleanup_old_analysis_results(
    self,
    days_to_keep: int = 90,
    batch_size: int = 1000
) -> Dict[str, Any]:
    """
    –û–ß–ò–°–¢–ö–ê –°–¢–ê–†–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í: –£–¥–∞–ª–µ–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
    
    Args:
        days_to_keep: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 90)
        batch_size: –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—á–∏—Å—Ç–∫–∏
    """
    logger.info(f"üßπ –ó–∞–ø—É—Å–∫ –æ—á–∏—Å—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ä—à–µ {days_to_keep} –¥–Ω–µ–π")
    
    with get_db_session() as db:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –æ—Ç—Å–µ—á–µ–Ω–∏—è
            cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            old_results_count = db.query(reranker_analysis_result_crud.model).filter(
                reranker_analysis_result_crud.model.created_at < cutoff_date
            ).count()
            
            old_sessions_count = db.query(reranker_analysis_session_crud.model).filter(
                reranker_analysis_session_crud.model.created_at < cutoff_date
            ).count()
            
            if old_results_count == 0 and old_sessions_count == 0:
                logger.info("‚úÖ –ù–µ—Ç —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
                return serialize_for_json({
                    'deleted_results': 0,
                    'deleted_sessions': 0,
                    'cutoff_date': cutoff_date.isoformat(),
                    'message': '–ù–µ—Ç —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è'
                })
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–∫–µ—Ç–∞–º–∏
            deleted_results = 0
            while True:
                old_results = db.query(reranker_analysis_result_crud.model).filter(
                    reranker_analysis_result_crud.model.created_at < cutoff_date
                ).limit(batch_size).all()
                
                if not old_results:
                    break
                
                for result in old_results:
                    db.delete(result)
                deleted_results += len(old_results)
                db.commit()
                
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {len(old_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ (–≤—Å–µ–≥–æ: {deleted_results})")
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–µ—Å—Å–∏–∏ –ø–∞–∫–µ—Ç–∞–º–∏
            deleted_sessions = 0
            while True:
                old_sessions = db.query(reranker_analysis_session_crud.model).filter(
                    reranker_analysis_session_crud.model.created_at < cutoff_date
                ).limit(batch_size).all()
                
                if not old_sessions:
                    break
                
                for session in old_sessions:
                    db.delete(session)
                deleted_sessions += len(old_sessions)
                db.commit()
                
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {len(old_sessions)} —Å–µ—Å—Å–∏–π –∞–Ω–∞–ª–∏–∑–∞ (–≤—Å–µ–≥–æ: {deleted_sessions})")
            
            result = {
                'deleted_results': deleted_results,
                'deleted_sessions': deleted_sessions,
                'cutoff_date': cutoff_date.isoformat(),
                'days_kept': days_to_keep,
                'completed_at': datetime.utcnow().isoformat(),
                'message': f'–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É–¥–∞–ª–µ–Ω–æ {deleted_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ {deleted_sessions} —Å–µ—Å—Å–∏–π'
            }
            
            logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {deleted_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, {deleted_sessions} —Å–µ—Å—Å–∏–π")
            return serialize_for_json(result)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            # Retry –ª–æ–≥–∏–∫–∞
            if self.request.retries < self.max_retries:
                logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {self.request.retries + 1}/{self.max_retries}")
                raise self.retry(countdown=60)
            raise


@app.task(
    bind=True,
    name='tasks.analysis_tasks.save_analysis_results',
    soft_time_limit=300,  # 5 –º–∏–Ω—É—Ç
    time_limit=360,       # 6 –º–∏–Ω—É—Ç
    max_retries=3
)
def save_analysis_results(
    self,
    entity_id: str,
    analysis_data: Dict[str, Any],
    processing_type: str = 'general'
) -> Dict[str, Any]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ —Ü–µ–ø–æ—á–µ–∫ A –∏ B
    
    Args:
        entity_id: ID —Å—É—â–Ω–æ—Å—Ç–∏ (job_id –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ A, submission_id –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ B)
        analysis_data: –î–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞ (–≤–∫–ª—é—á–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –∏ AI)
        processing_type: –¢–∏–ø –æ–±—Ä–∞–±–æ—Ç–∫–∏ ('resume_processing', 'job_processing', 'general')
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {processing_type}: {entity_id}")
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ processing_type
        if processing_type == 'resume_processing':
            return _save_resume_analysis_results(entity_id, analysis_data)
        elif processing_type == 'job_processing':
            return _save_job_analysis_results(entity_id, analysis_data)
        else:
            return _save_general_analysis_results(entity_id, analysis_data, processing_type)
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        if self.request.retries < self.max_retries:
            logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {self.request.retries + 1}/{self.max_retries}")
            raise self.retry(countdown=60)
        raise


def _save_resume_analysis_results(job_id: str, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ (—Ü–µ–ø–æ—á–∫–∞ A)"""
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_id}")
    
    try:
        with get_db_session() as db_session:
            saved_count = 0
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
            matches = analysis_data.get('matches', [])
            for match in matches:
                try:
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    result_data = {
                        'job_id': job_id,
                        'submission_id': match.get('submission_id'),
                        'original_score': match.get('original_score', 0.0),
                        'rerank_score': match.get('rerank_score', 0.0),
                        'similarity_score': match.get('similarity_score', 0.0),
                        'metadata': {
                            'processing_type': 'resume_processing',
                            'analysis_timestamp': datetime.utcnow().isoformat(),
                            'match_data': match
                        }
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º AI –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –µ—Å—Ç—å
                    ai_analysis = analysis_data.get('ai_analysis')
                    if ai_analysis:
                        result_data['metadata']['ai_analysis'] = ai_analysis
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î —á–µ—Ä–µ–∑ CRUD –æ–ø–µ—Ä–∞—Ü–∏—é
                    result_crud = reranker_analysis_result_crud.create(db_session, result_data)
                    if result_crud:
                        saved_count += 1
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è match –¥–ª—è submission {match.get('submission_id')}: {e}")
                    continue
        
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ")
        return {
            'status': 'success',
            'saved_count': saved_count,
            'job_id': job_id,
            'processing_type': 'resume_processing'
        }
    
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ–∑—é–º–µ: {e}")
        raise


def _save_job_analysis_results(submission_id: str, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–π (—Ü–µ–ø–æ—á–∫–∞ B)"""
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è submission {submission_id}")
    
    try:
        with get_db_session() as db_session:
            saved_count = 0
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
            matches = analysis_data.get('matches', [])
            for match in matches:
                try:
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    result_data = {
                        'submission_id': submission_id,
                        'job_id': match.get('job_id'),
                        'original_score': match.get('original_score', 0.0),
                        'rerank_score': match.get('rerank_score', 0.0),
                        'similarity_score': match.get('similarity_score', 0.0),
                        'metadata': {
                            'processing_type': 'job_processing',
                            'analysis_timestamp': datetime.utcnow().isoformat(),
                            'match_data': match
                        }
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º AI –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –µ—Å—Ç—å
                    ai_analysis = analysis_data.get('ai_analysis')
                    if ai_analysis:
                        result_data['metadata']['ai_analysis'] = ai_analysis
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î —á–µ—Ä–µ–∑ CRUD –æ–ø–µ—Ä–∞—Ü–∏—é
                    result_crud = reranker_analysis_result_crud.create(db_session, result_data)
                    if result_crud:
                        saved_count += 1
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è match –¥–ª—è job {match.get('job_id')}: {e}")
                    continue
        
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–π")
        return {
            'status': 'success',
            'saved_count': saved_count,
            'submission_id': submission_id,
            'processing_type': 'job_processing'
        }
    
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–∫–∞–Ω—Å–∏–π: {e}")
        raise


def _save_general_analysis_results(entity_id: str, analysis_data: Dict[str, Any], processing_type: str) -> Dict[str, Any]:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ {processing_type} –¥–ª—è {entity_id}")
    
    try:
        with get_db_session() as db_session:
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞
            session_data = {
                'entity_id': entity_id,
                'processing_type': processing_type,
                'metadata': {
                    'analysis_data': analysis_data,
                    'timestamp': datetime.utcnow().isoformat()
                }
            }
            
            session_crud = reranker_analysis_session_crud.create(db_session, session_data)
        
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –æ–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {entity_id}")
        return {
            'status': 'success',
            'entity_id': entity_id,
            'processing_type': processing_type,
            'session_id': session_crud.id if session_crud else None
        }
    
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        raise
