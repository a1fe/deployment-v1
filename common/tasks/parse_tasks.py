"""
Document parsing tasks for HR analysis system
"""

import os
import hashlib
import tempfile
import requests
from typing import Dict, List, Any, Optional
from celery import Celery
from celery.utils.log import get_task_logger
from datetime import datetime

# PDF and document processing libraries
try:
    from PyPDF2 import PdfReader
    from docx import Document
    import docx2txt
except ImportError:
    logger = get_task_logger(__name__)
    logger.warning("‚ö†Ô∏è Document processing libraries not available")

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä Celery
# –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis —á–µ—Ä–µ–∑ Secret Manager
from deployment.common.utils.secret_manager import get_redis_url_with_auth
redis_url = get_redis_url_with_auth()
app = Celery('hr_analysis', broker=redis_url, backend=redis_url)
logger = get_task_logger(__name__)


@app.task(
    bind=True,
    name='tasks.parse_tasks.parse_documents',
    soft_time_limit=600,  # 10 –º–∏–Ω—É—Ç
    time_limit=720,       # 12 –º–∏–Ω—É—Ç
    max_retries=3
)
def parse_documents(self, documents_data: List[Dict[str, Any]], document_type: str = 'resume') -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–µ–∑—é–º–µ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π)
    
    Args:
        documents_data: –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å URL –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        document_type: –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ('resume' –∏–ª–∏ 'job_description')
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
    """
    logger.info(f"üìÑ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ {len(documents_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–∏–ø–∞ '{document_type}'")
    
    try:
        parsed_documents = []
        failed_documents = []
        
        for i, doc_data in enumerate(documents_data):
            try:
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress = (i / len(documents_data)) * 100
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'progress': int(progress),
                        'status': f'–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {i+1}/{len(documents_data)}',
                        'document_type': document_type
                    }
                )
                
                # –ü–∞—Ä—Å–∏–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                parsed_doc = _parse_single_document(doc_data, document_type)
                if parsed_doc:
                    parsed_documents.append(parsed_doc)
                else:
                    failed_documents.append({
                        'document_id': doc_data.get('id', f'doc_{i}'),
                        'error': 'Failed to parse document'
                    })
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {i+1}: {e}")
                failed_documents.append({
                    'document_id': doc_data.get('id', f'doc_{i}'),
                    'error': str(e)
                })
                continue
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.update_state(
            state='SUCCESS',
            meta={
                'progress': 100,
                'status': '–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω',
                'parsed_count': len(parsed_documents),
                'failed_count': len(failed_documents)
            }
        )
        
        result = {
            'status': 'completed',
            'document_type': document_type,
            'parsed_documents': parsed_documents,
            'failed_documents': failed_documents,
            'stats': {
                'total_count': len(documents_data),
                'parsed_count': len(parsed_documents),
                'failed_count': len(failed_documents),
                'success_rate': len(parsed_documents) / len(documents_data) * 100 if documents_data else 0
            },
            'processed_at': datetime.utcnow().isoformat()
        }
        
        logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(parsed_documents)} —É—Å–ø–µ—à–Ω–æ, {len(failed_documents)} –æ—à–∏–±–æ–∫")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'document_type': document_type,
            'processed_at': datetime.utcnow().isoformat()
        }


def _parse_single_document(doc_data: Dict[str, Any], document_type: str) -> Optional[Dict[str, Any]]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    Args:
        doc_data: –î–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–≤–∫–ª—é—á–∞—è URL, ID, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        document_type: –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
    Returns:
        –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    try:
        doc_id = doc_data.get('id') or doc_data.get('submission_id') or doc_data.get('job_id')
        doc_url = doc_data.get('url') or doc_data.get('file_url') or doc_data.get('resume_file_url')
        
        if not doc_url:
            logger.warning(f"‚ö†Ô∏è URL –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {doc_id}")
            return None
        
        logger.info(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {_mask_url(doc_url)}")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        response = requests.get(doc_url, timeout=30)
        response.raise_for_status()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
        content_type = response.headers.get('content-type', '').lower()
        file_extension = _get_file_extension(doc_url, content_type)
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as temp_file:
            temp_file.write(response.content)
            temp_file_path = temp_file.name
        
        try:
            # –ü–∞—Ä—Å–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
            if file_extension == '.pdf':
                text_content = _parse_pdf(temp_file_path)
            elif file_extension in ['.doc', '.docx']:
                text_content = _parse_docx(temp_file_path)
            else:
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∫ —Ç–µ–∫—Å—Ç
                with open(temp_file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text_content = f.read()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            content_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if document_type == 'resume':
                structured_data = _extract_resume_info(text_content, doc_data)
            elif document_type == 'job_description':
                structured_data = _extract_job_info(text_content, doc_data)
            else:
                structured_data = {}
            
            parsed_doc = {
                'id': doc_id,
                'type': document_type,
                'url': doc_url,
                'content_hash': content_hash,
                'text_content': text_content,
                'structured_data': structured_data,
                'metadata': {
                    'file_size': len(response.content),
                    'content_type': content_type,
                    'file_extension': file_extension,
                    'parsed_at': datetime.utcnow().isoformat()
                },
                'original_data': doc_data
            }
            
            return parsed_doc
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")
        return None


def _parse_pdf(file_path: str) -> str:
    """–ü–∞—Ä—Å–∏–Ω–≥ PDF —Ñ–∞–π–ª–∞"""
    try:
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF: {e}")
        raise


def _parse_docx(file_path: str) -> str:
    """–ü–∞—Ä—Å–∏–Ω–≥ DOCX —Ñ–∞–π–ª–∞"""
    try:
        # –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ docx2txt (–±—ã—Å—Ç—Ä–µ–µ)
        text = docx2txt.process(file_path)
        if text.strip():
            return text.strip()
        
        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º python-docx
        doc = Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ DOCX: {e}")
        raise


def _extract_resume_info(text_content: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–µ–∑—é–º–µ"""
    # –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é NLP
    structured_info = {
        'skills': _extract_skills(text_content),
        'experience': _extract_experience(text_content),
        'education': _extract_education(text_content),
        'contact_info': _extract_contact_info(text_content),
        'summary': _extract_summary(text_content)
    }
    
    # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã
    if original_data:
        structured_info.update({
            'form_data': {
                'first_name': original_data.get('first_name'),
                'last_name': original_data.get('last_name'),
                'email': original_data.get('email'),
                'phone': original_data.get('phone'),
                'skills': original_data.get('skills'),
                'experience_years': original_data.get('experience_years'),
                'desired_position': original_data.get('desired_position')
            }
        })
    
    return structured_info


def _extract_job_info(text_content: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏"""
    structured_info = {
        'requirements': _extract_requirements(text_content),
        'responsibilities': _extract_responsibilities(text_content),
        'benefits': _extract_benefits(text_content),
        'qualifications': _extract_qualifications(text_content),
        'company_info': _extract_company_info(text_content)
    }
    
    # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã
    if original_data:
        structured_info.update({
            'form_data': {
                'job_title': original_data.get('job_title'),
                'company_name': original_data.get('company_name'),
                'location': original_data.get('location'),
                'salary_range': original_data.get('salary_range'),
                'employment_type': original_data.get('employment_type'),
                'is_remote': original_data.get('is_remote')
            }
        })
    
    return structured_info


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
def _extract_skills(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –ø–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    skills_keywords = ['python', 'javascript', 'java', 'c++', 'sql', 'html', 'css', 'react', 'angular', 'vue', 'docker', 'kubernetes']
    found_skills = []
    text_lower = text.lower()
    for skill in skills_keywords:
        if skill in text_lower:
            found_skills.append(skill)
    return found_skills


def _extract_experience(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã"""
    # –ü–æ–∏—Å–∫ —Å–µ–∫—Ü–∏–π —Å –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã
    experience_sections = []
    lines = text.split('\n')
    current_section = []
    
    for line in lines:
        line = line.strip()
        if any(keyword in line.lower() for keyword in ['experience', 'work', 'employment', 'position']):
            if current_section:
                experience_sections.append('\n'.join(current_section))
                current_section = []
        current_section.append(line)
    
    if current_section:
        experience_sections.append('\n'.join(current_section))
    
    return experience_sections


def _extract_education(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""
    education_keywords = ['university', 'college', 'degree', 'bachelor', 'master', 'phd', 'education']
    education_info = []
    lines = text.split('\n')
    
    for line in lines:
        if any(keyword in line.lower() for keyword in education_keywords):
            education_info.append(line.strip())
    
    return education_info


def _extract_contact_info(text: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    import re
    contact_info = {}
    
    # Email
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails = re.findall(email_pattern, text)
    if emails:
        contact_info['emails'] = emails
    
    # Phone
    phone_pattern = r'[\+]?[1-9]?[0-9]{7,15}'
    phones = re.findall(phone_pattern, text)
    if phones:
        contact_info['phones'] = phones
    
    return contact_info


def _extract_summary(text: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ/–æ–ø–∏—Å–∞–Ω–∏—è"""
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –∫–∞–∫ —Ä–µ–∑—é–º–µ
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    if lines:
        return ' '.join(lines[:3])  # –ü–µ—Ä–≤—ã–µ 3 –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    return ""


def _extract_requirements(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏"""
    requirements = []
    lines = text.split('\n')
    in_requirements = False
    
    for line in lines:
        line = line.strip()
        if any(keyword in line.lower() for keyword in ['requirements', 'qualifications', 'must have']):
            in_requirements = True
            continue
        if in_requirements and line and not line.startswith('‚Ä¢') and not line.startswith('-'):
            if any(keyword in line.lower() for keyword in ['responsibilities', 'benefits', 'we offer']):
                break
        if in_requirements and line:
            requirements.append(line)
    
    return requirements


def _extract_responsibilities(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π"""
    responsibilities = []
    lines = text.split('\n')
    in_responsibilities = False
    
    for line in lines:
        line = line.strip()
        if any(keyword in line.lower() for keyword in ['responsibilities', 'duties', 'you will']):
            in_responsibilities = True
            continue
        if in_responsibilities and line and not line.startswith('‚Ä¢') and not line.startswith('-'):
            if any(keyword in line.lower() for keyword in ['requirements', 'benefits', 'qualifications']):
                break
        if in_responsibilities and line:
            responsibilities.append(line)
    
    return responsibilities


def _extract_benefits(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª—å–≥–æ—Ç –∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤"""
    benefits = []
    lines = text.split('\n')
    in_benefits = False
    
    for line in lines:
        line = line.strip()
        if any(keyword in line.lower() for keyword in ['benefits', 'we offer', 'perks', 'compensation']):
            in_benefits = True
            continue
        if in_benefits and line and not line.startswith('‚Ä¢') and not line.startswith('-'):
            if any(keyword in line.lower() for keyword in ['requirements', 'responsibilities']):
                break
        if in_benefits and line:
            benefits.append(line)
    
    return benefits


def _extract_qualifications(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–π"""
    return _extract_requirements(text)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É


def _extract_company_info(text: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏"""
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
    return {
        'description': _extract_summary(text)
    }


def _get_file_extension(url: str, content_type: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞"""
    if 'pdf' in content_type:
        return '.pdf'
    elif 'word' in content_type or 'document' in content_type:
        return '.docx'
    elif url.endswith('.pdf'):
        return '.pdf'
    elif url.endswith(('.doc', '.docx')):
        return '.docx'
    else:
        return '.txt'


def _mask_url(url: str) -> str:
    """–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ URL –¥–ª—è –ª–æ–≥–æ–≤"""
    if len(url) > 50:
        return url[:25] + "..." + url[-20:]
    return url
