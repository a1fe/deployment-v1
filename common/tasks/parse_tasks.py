"""
Document parsing tasks for HR analysis system
"""

import os
import hashlib
import tempfile
import requests
from typing import Dict, List, Any, Optional
from celery import Celery
from celery.utils.log import get_task_logger
from datetime import datetime

# PDF and document processing libraries
try:
    from PyPDF2 import PdfReader
    from docx import Document
    import docx2txt
except ImportError:
    logger = get_task_logger(__name__)
    logger.warning("âš ï¸ Document processing libraries not available")

# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Celery
# Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº Redis Ñ‡ĞµÑ€ĞµĞ· Secret Manager
from deployment.common.utils.secret_manager import get_redis_url_with_auth
redis_url = get_redis_url_with_auth()
app = Celery('hr_analysis', broker=redis_url, backend=redis_url)
logger = get_task_logger(__name__)


@app.task(
    bind=True,
    name='tasks.parse_tasks.parse_documents',
    soft_time_limit=600,  # 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚
    time_limit=720,       # 12 Ğ¼Ğ¸Ğ½ÑƒÑ‚
    max_retries=3
)
def parse_documents(self, documents_data: List[Dict[str, Any]], document_type: str = 'resume') -> Dict[str, Any]:
    """
    ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¸Ğ»Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹)
    
    Args:
        documents_data: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ URL Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
        document_type: Ğ¢Ğ¸Ğ¿ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ('resume' Ğ¸Ğ»Ğ¸ 'job_description')
        
    Returns:
        Dict Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°
    """
    logger.info(f"ğŸ“„ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° {len(documents_data)} Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° '{document_type}'")
    
    try:
        parsed_documents = []
        failed_documents = []
        
        for i, doc_data in enumerate(documents_data):
            try:
                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ
                progress = (i / len(documents_data)) * 100
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'progress': int(progress),
                        'status': f'ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° {i+1}/{len(documents_data)}',
                        'document_type': document_type
                    }
                )
                
                # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
                parsed_doc = _parse_single_document(doc_data, document_type)
                if parsed_doc:
                    parsed_documents.append(parsed_doc)
                else:
                    failed_documents.append({
                        'document_id': doc_data.get('id', f'doc_{i}'),
                        'error': 'Failed to parse document'
                    })
                    
            except Exception as e:
                logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° {i+1}: {e}")
                failed_documents.append({
                    'document_id': doc_data.get('id', f'doc_{i}'),
                    'error': str(e)
                })
                continue
        
        # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°
        self.update_state(
            state='SUCCESS',
            meta={
                'progress': 100,
                'status': 'ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½',
                'parsed_count': len(parsed_documents),
                'failed_count': len(failed_documents)
            }
        )
        
        result = {
            'status': 'completed',
            'document_type': document_type,
            'parsed_documents': parsed_documents,
            'failed_documents': failed_documents,
            'stats': {
                'total_count': len(documents_data),
                'parsed_count': len(parsed_documents),
                'failed_count': len(failed_documents),
                'success_rate': len(parsed_documents) / len(documents_data) * 100 if documents_data else 0
            },
            'processed_at': datetime.utcnow().isoformat()
        }
        
        logger.info(f"âœ… ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½: {len(parsed_documents)} ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾, {len(failed_documents)} Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'document_type': document_type,
            'processed_at': datetime.utcnow().isoformat()
        }


def _parse_single_document(doc_data: Dict[str, Any], document_type: str) -> Optional[Dict[str, Any]]:
    """
    ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
    
    Args:
        doc_data: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ URL, ID, Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ)
        document_type: Ğ¢Ğ¸Ğ¿ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
        
    Returns:
        Ğ Ğ°ÑĞ¿Ğ°Ñ€ÑĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸Ğ»Ğ¸ None Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ
    """
    try:
        doc_id = doc_data.get('id') or doc_data.get('submission_id') or doc_data.get('job_id')
        doc_url = doc_data.get('url') or doc_data.get('file_url') or doc_data.get('resume_file_url')
        
        if not doc_url:
            logger.warning(f"âš ï¸ URL Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ´Ğ»Ñ {doc_id}")
            return None
        
        logger.info(f"ğŸ“„ ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° {doc_id}: {_mask_url(doc_url)}")
        
        # Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
        response = requests.get(doc_url, timeout=30)
        response.raise_for_status()
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ñ„Ğ°Ğ¹Ğ»Ğ°
        content_type = response.headers.get('content-type', '').lower()
        file_extension = _get_file_extension(doc_url, content_type)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»
        with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as temp_file:
            temp_file.write(response.content)
            temp_file_path = temp_file.name
        
        try:
            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ñ„Ğ°Ğ¹Ğ»Ğ°
            if file_extension == '.pdf':
                text_content = _parse_pdf(temp_file_path)
            elif file_extension in ['.doc', '.docx']:
                text_content = _parse_docx(temp_file_path)
            else:
                # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚
                with open(temp_file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text_content = f.read()
            
            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ñ…ÑÑˆ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²
            content_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
            if document_type == 'resume':
                structured_data = _extract_resume_info(text_content, doc_data)
            elif document_type == 'job_description':
                structured_data = _extract_job_info(text_content, doc_data)
            else:
                structured_data = {}
            
            parsed_doc = {
                'id': doc_id,
                'type': document_type,
                'url': doc_url,
                'content_hash': content_hash,
                'text_content': text_content,
                'structured_data': structured_data,
                'metadata': {
                    'file_size': len(response.content),
                    'content_type': content_type,
                    'file_extension': file_extension,
                    'parsed_at': datetime.utcnow().isoformat()
                },
                'original_data': doc_data
            }
            
            return parsed_doc
            
        finally:
            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
    
    except Exception as e:
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° {doc_id}: {e}")
        return None


def _parse_pdf(file_path: str) -> str:
    """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ PDF Ñ„Ğ°Ğ¹Ğ»Ğ°"""
    try:
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° PDF: {e}")
        raise


def _parse_docx(file_path: str) -> str:
    """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ DOCX Ñ„Ğ°Ğ¹Ğ»Ğ°"""
    try:
        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° docx2txt (Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ)
        text = docx2txt.process(file_path)
        if text.strip():
            return text.strip()
        
        # Ğ•ÑĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ python-docx
        doc = Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° DOCX: {e}")
        raise


def _extract_resume_info(text_content: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€ĞµĞ·ÑĞ¼Ğµ"""
    # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ NLP
    structured_info = {
        'skills': _extract_skills(text_content),
        'experience': _extract_experience(text_content),
        'education': _extract_education(text_content),
        'contact_info': _extract_contact_info(text_content),
        'summary': _extract_summary(text_content)
    }
    
    # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹
    if original_data:
        structured_info.update({
            'form_data': {
                'first_name': original_data.get('first_name'),
                'last_name': original_data.get('last_name'),
                'email': original_data.get('email'),
                'phone': original_data.get('phone'),
                'skills': original_data.get('skills'),
                'experience_years': original_data.get('experience_years'),
                'desired_position': original_data.get('desired_position')
            }
        })
    
    return structured_info


def _extract_job_info(text_content: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸"""
    structured_info = {
        'requirements': _extract_requirements(text_content),
        'responsibilities': _extract_responsibilities(text_content),
        'benefits': _extract_benefits(text_content),
        'qualifications': _extract_qualifications(text_content),
        'company_info': _extract_company_info(text_content)
    }
    
    # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹
    if original_data:
        structured_info.update({
            'form_data': {
                'job_title': original_data.get('job_title'),
                'company_name': original_data.get('company_name'),
                'location': original_data.get('location'),
                'salary_range': original_data.get('salary_range'),
                'employment_type': original_data.get('employment_type'),
                'is_remote': original_data.get('is_remote')
            }
        })
    
    return structured_info


# Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸
def _extract_skills(text: str) -> List[str]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
    # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - Ğ¿Ğ¾Ğ¸ÑĞº ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
    skills_keywords = ['python', 'javascript', 'java', 'c++', 'sql', 'html', 'css', 'react', 'angular', 'vue', 'docker', 'kubernetes']
    found_skills = []
    text_lower = text.lower()
    for skill in skills_keywords:
        if skill in text_lower:
            found_skills.append(skill)
    return found_skills


def _extract_experience(text: str) -> List[str]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹"""
    # ĞŸĞ¾Ğ¸ÑĞº ÑĞµĞºÑ†Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹
    experience_sections = []
    lines = text.split('\n')
    current_section = []
    
    for line in lines:
        line = line.strip()
        if any(keyword in line.lower() for keyword in ['experience', 'work', 'employment', 'position']):
            if current_section:
                experience_sections.append('\n'.join(current_section))
                current_section = []
        current_section.append(line)
    
    if current_section:
        experience_sections.append('\n'.join(current_section))
    
    return experience_sections


def _extract_education(text: str) -> List[str]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""
    education_keywords = ['university', 'college', 'degree', 'bachelor', 'master', 'phd', 'education']
    education_info = []
    lines = text.split('\n')
    
    for line in lines:
        if any(keyword in line.lower() for keyword in education_keywords):
            education_info.append(line.strip())
    
    return education_info


def _extract_contact_info(text: str) -> Dict[str, Any]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸"""
    import re
    contact_info = {}
    
    # Email
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails = re.findall(email_pattern, text)
    if emails:
        contact_info['emails'] = emails
    
    # Phone
    phone_pattern = r'[\+]?[1-9]?[0-9]{7,15}'
    phones = re.findall(phone_pattern, text)
    if phones:
        contact_info['phones'] = phones
    
    return contact_info


def _extract_summary(text: str) -> str:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ/Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ"""
    # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ¾Ğº ĞºĞ°Ğº Ñ€ĞµĞ·ÑĞ¼Ğµ
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    if lines:
        return ' '.join(lines[:3])  # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 3 Ğ½ĞµĞ¿ÑƒÑÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
    return ""


def _extract_requirements(text: str) -> List[str]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸"""
    requirements = []
    lines = text.split('\n')
    in_requirements = False
    
    for line in lines:
        line = line.strip()
        if any(keyword in line.lower() for keyword in ['requirements', 'qualifications', 'must have']):
            in_requirements = True
            continue
        if in_requirements and line and not line.startswith('â€¢') and not line.startswith('-'):
            if any(keyword in line.lower() for keyword in ['responsibilities', 'benefits', 'we offer']):
                break
        if in_requirements and line:
            requirements.append(line)
    
    return requirements


def _extract_responsibilities(text: str) -> List[str]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹"""
    responsibilities = []
    lines = text.split('\n')
    in_responsibilities = False
    
    for line in lines:
        line = line.strip()
        if any(keyword in line.lower() for keyword in ['responsibilities', 'duties', 'you will']):
            in_responsibilities = True
            continue
        if in_responsibilities and line and not line.startswith('â€¢') and not line.startswith('-'):
            if any(keyword in line.lower() for keyword in ['requirements', 'benefits', 'qualifications']):
                break
        if in_responsibilities and line:
            responsibilities.append(line)
    
    return responsibilities


def _extract_benefits(text: str) -> List[str]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ»ÑŒĞ³Ğ¾Ñ‚ Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²"""
    benefits = []
    lines = text.split('\n')
    in_benefits = False
    
    for line in lines:
        line = line.strip()
        if any(keyword in line.lower() for keyword in ['benefits', 'we offer', 'perks', 'compensation']):
            in_benefits = True
            continue
        if in_benefits and line and not line.startswith('â€¢') and not line.startswith('-'):
            if any(keyword in line.lower() for keyword in ['requirements', 'responsibilities']):
                break
        if in_benefits and line:
            benefits.append(line)
    
    return benefits


def _extract_qualifications(text: str) -> List[str]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ²Ğ°Ğ»Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹"""
    return _extract_requirements(text)  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚Ñƒ Ğ¶Ğµ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ


def _extract_company_info(text: str) -> Dict[str, Any]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸"""
    # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    return {
        'description': _extract_summary(text)
    }


def _get_file_extension(url: str, content_type: str) -> str:
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°"""
    if 'pdf' in content_type:
        return '.pdf'
    elif 'word' in content_type or 'document' in content_type:
        return '.docx'
    elif url.endswith('.pdf'):
        return '.pdf'
    elif url.endswith(('.doc', '.docx')):
        return '.docx'
    else:
        return '.txt'


def _mask_url(url: str) -> str:
    """ĞœĞ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ URL Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¾Ğ²"""
    if len(url) > 50:
        return url[:25] + "..." + url[-20:]
    return url
